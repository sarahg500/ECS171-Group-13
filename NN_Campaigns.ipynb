{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b0325b",
   "metadata": {
    "id": "happy-automation",
    "outputId": "6b08a0d1-51ce-4146-f26a-f097e4edd6b8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import csv\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('marketing_campaign.csv')\n",
    "\n",
    "df.drop(df.index[[2233,239,339,192,21,1975,1806,1328]], inplace=True)  # Remove outliers\n",
    "df = df.dropna() # Removing rows with missing values\n",
    "df.drop(df[df.Marital_Status == \"YOLO\"].index, inplace=True)\n",
    "df.drop(df[df.Marital_Status == \"Absurd\"].index, inplace=True)\n",
    "df = df[df['Income'] <= 100000]\n",
    "\n",
    "marital = pd.get_dummies(df['Marital_Status'], drop_first=True)\n",
    "df = pd.concat([df, marital], axis=1)\n",
    "\n",
    "\n",
    "education = pd.get_dummies(df['Education'], drop_first=True)\n",
    "df = pd.concat([df, education], axis=1)\n",
    "\n",
    "df = df.drop(columns=['Education','Marital_Status'])\n",
    "\n",
    "# Filling NA columns with data mean since it is well distributed after outliers removed\n",
    "df['Income'] = df['Income'].fillna(df['Income'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc59802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Recency',\n",
      "       'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
      "       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n",
      "       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
      "       'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5',\n",
      "       'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Response', 'Divorced',\n",
      "       'Married', 'Single', 'Together', 'Widow', 'Basic', 'Graduation',\n",
      "       'Master', 'PhD', 'Year_joined', 'Month_joined', 'Day_joined'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.drop(columns = ['Z_CostContact', 'Z_Revenue'])\n",
    "\n",
    "# Changing Date numerically readable values  \n",
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'])\n",
    "df['Year_joined'] = df['Dt_Customer'].apply(lambda x: x.year)\n",
    "df['Month_joined'] = df['Dt_Customer'].apply(lambda x: x.month)\n",
    "df['Day_joined'] = df['Dt_Customer'].apply(lambda x: x.day)\n",
    "df = df.drop(columns = ['Dt_Customer'])\n",
    "\n",
    "print(df.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d08e26",
   "metadata": {
    "id": "ebe7fe6e",
    "outputId": "bd27b7cc-1320-4818-8d72-b06f64b0d2ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>...</th>\n",
       "      <th>Single</th>\n",
       "      <th>Together</th>\n",
       "      <th>Widow</th>\n",
       "      <th>Basic</th>\n",
       "      <th>Graduation</th>\n",
       "      <th>Master</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Year_joined</th>\n",
       "      <th>Month_joined</th>\n",
       "      <th>Day_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_Birth   Income  Kidhome  Teenhome  Recency  MntWines  MntFruits  \\\n",
       "0          58  58138.0        0         0       58       635         88   \n",
       "1          61  46344.0        1         1       38        11          1   \n",
       "2          50  71613.0        0         0       26       426         49   \n",
       "3          31  26646.0        1         0       26        11          4   \n",
       "4          34  58293.0        1         0       94       173         43   \n",
       "\n",
       "   MntMeatProducts  MntFishProducts  MntSweetProducts  ...  Single  Together  \\\n",
       "0              546              172                88  ...       1         0   \n",
       "1                6                2                 1  ...       1         0   \n",
       "2              127              111                21  ...       0         1   \n",
       "3               20               10                 3  ...       0         1   \n",
       "4              118               46                27  ...       0         0   \n",
       "\n",
       "   Widow  Basic  Graduation  Master  PhD  Year_joined  Month_joined  \\\n",
       "0      0      0           1       0    0         2012             4   \n",
       "1      0      0           1       0    0         2014             8   \n",
       "2      0      0           1       0    0         2013             8   \n",
       "3      0      0           1       0    0         2014            10   \n",
       "4      0      0           0       0    1         2014             1   \n",
       "\n",
       "   Day_joined  \n",
       "0           9  \n",
       "1           3  \n",
       "2          21  \n",
       "3           2  \n",
       "4          19  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfram = df.copy()\n",
    "dfram = dfram.drop('ID', axis=1)\n",
    "\n",
    "dfram['Year_Birth'] = dfram['Year_Birth'].map(lambda a: 2015-a)\n",
    "dfram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfa9a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>...</th>\n",
       "      <th>Single</th>\n",
       "      <th>Together</th>\n",
       "      <th>Widow</th>\n",
       "      <th>Basic</th>\n",
       "      <th>Graduation</th>\n",
       "      <th>Master</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Year_joined</th>\n",
       "      <th>Month_joined</th>\n",
       "      <th>Day_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>41</td>\n",
       "      <td>34421.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>48</td>\n",
       "      <td>61223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>709</td>\n",
       "      <td>43</td>\n",
       "      <td>182</td>\n",
       "      <td>42</td>\n",
       "      <td>118</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>69</td>\n",
       "      <td>64014.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>34</td>\n",
       "      <td>56981.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>908</td>\n",
       "      <td>48</td>\n",
       "      <td>217</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>59</td>\n",
       "      <td>69245.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>428</td>\n",
       "      <td>30</td>\n",
       "      <td>214</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2191 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year_Birth   Income  Kidhome  Teenhome  Recency  MntWines  MntFruits  \\\n",
       "0             58  58138.0        0         0       58       635         88   \n",
       "1             61  46344.0        1         1       38        11          1   \n",
       "2             50  71613.0        0         0       26       426         49   \n",
       "3             31  26646.0        1         0       26        11          4   \n",
       "4             34  58293.0        1         0       94       173         43   \n",
       "...          ...      ...      ...       ...      ...       ...        ...   \n",
       "2234          41  34421.0        1         0       81         3          3   \n",
       "2235          48  61223.0        0         1       46       709         43   \n",
       "2236          69  64014.0        2         1       56       406          0   \n",
       "2237          34  56981.0        0         0       91       908         48   \n",
       "2238          59  69245.0        0         1        8       428         30   \n",
       "\n",
       "      MntMeatProducts  MntFishProducts  MntSweetProducts  ...  Single  \\\n",
       "0                 546              172                88  ...       1   \n",
       "1                   6                2                 1  ...       1   \n",
       "2                 127              111                21  ...       0   \n",
       "3                  20               10                 3  ...       0   \n",
       "4                 118               46                27  ...       0   \n",
       "...               ...              ...               ...  ...     ...   \n",
       "2234                7                6                 2  ...       0   \n",
       "2235              182               42               118  ...       0   \n",
       "2236               30                0                 0  ...       0   \n",
       "2237              217               32                12  ...       0   \n",
       "2238              214               80                30  ...       0   \n",
       "\n",
       "      Together  Widow  Basic  Graduation  Master  PhD  Year_joined  \\\n",
       "0            0      0      0           1       0    0         2012   \n",
       "1            0      0      0           1       0    0         2014   \n",
       "2            1      0      0           1       0    0         2013   \n",
       "3            1      0      0           1       0    0         2014   \n",
       "4            0      0      0           0       0    1         2014   \n",
       "...        ...    ...    ...         ...     ...  ...          ...   \n",
       "2234         0      0      0           1       0    0         2013   \n",
       "2235         0      0      0           1       0    0         2013   \n",
       "2236         1      0      0           0       0    1         2014   \n",
       "2237         0      0      0           1       0    0         2014   \n",
       "2238         1      0      0           0       1    0         2014   \n",
       "\n",
       "      Month_joined  Day_joined  \n",
       "0                4           9  \n",
       "1                8           3  \n",
       "2                8          21  \n",
       "3               10           2  \n",
       "4                1          19  \n",
       "...            ...         ...  \n",
       "2234             1           7  \n",
       "2235             6          13  \n",
       "2236            10           6  \n",
       "2237             1          25  \n",
       "2238             1          24  \n",
       "\n",
       "[2191 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfram.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccb082a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>...</th>\n",
       "      <th>Single</th>\n",
       "      <th>Together</th>\n",
       "      <th>Widow</th>\n",
       "      <th>Basic</th>\n",
       "      <th>Graduation</th>\n",
       "      <th>Master</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Year_joined</th>\n",
       "      <th>Month_joined</th>\n",
       "      <th>Day_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.581244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.425318</td>\n",
       "      <td>0.442211</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.664093</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.459715</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.720094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.285332</td>\n",
       "      <td>0.246231</td>\n",
       "      <td>0.129065</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.256742</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.038610</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.582841</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.115874</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.119919</td>\n",
       "      <td>0.177606</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.613033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.464646</td>\n",
       "      <td>0.474883</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.184959</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.641792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.271936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.569322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.608171</td>\n",
       "      <td>0.241206</td>\n",
       "      <td>0.220528</td>\n",
       "      <td>0.123552</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.695694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.286671</td>\n",
       "      <td>0.150754</td>\n",
       "      <td>0.217480</td>\n",
       "      <td>0.308880</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.526951</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.056263</td>\n",
       "      <td>0.015075</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2192 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year_Birth    Income  Kidhome  Teenhome   Recency  MntWines  MntFruits  \\\n",
       "0       0.696429  0.581244      0.0       0.0  0.585859  0.425318   0.442211   \n",
       "1       0.750000  0.459715      0.5       0.5  0.383838  0.007368   0.005025   \n",
       "2       0.553571  0.720094      0.0       0.0  0.262626  0.285332   0.246231   \n",
       "3       0.214286  0.256742      0.5       0.0  0.262626  0.007368   0.020101   \n",
       "4       0.267857  0.582841      0.5       0.0  0.949495  0.115874   0.216080   \n",
       "...          ...       ...      ...       ...       ...       ...        ...   \n",
       "2235    0.517857  0.613033      0.0       0.5  0.464646  0.474883   0.216080   \n",
       "2236    0.892857  0.641792      1.0       0.5  0.565657  0.271936   0.000000   \n",
       "2237    0.267857  0.569322      0.0       0.0  0.919192  0.608171   0.241206   \n",
       "2238    0.714286  0.695694      0.0       0.5  0.080808  0.286671   0.150754   \n",
       "2239    0.750000  0.526951      0.5       0.5  0.404040  0.056263   0.015075   \n",
       "\n",
       "      MntMeatProducts  MntFishProducts  MntSweetProducts  ...  Single  \\\n",
       "0            0.554878         0.664093          0.444444  ...     1.0   \n",
       "1            0.006098         0.007722          0.005051  ...     1.0   \n",
       "2            0.129065         0.428571          0.106061  ...     0.0   \n",
       "3            0.020325         0.038610          0.015152  ...     0.0   \n",
       "4            0.119919         0.177606          0.136364  ...     0.0   \n",
       "...               ...              ...               ...  ...     ...   \n",
       "2235         0.184959         0.162162          0.595960  ...     0.0   \n",
       "2236         0.030488         0.000000          0.000000  ...     0.0   \n",
       "2237         0.220528         0.123552          0.060606  ...     0.0   \n",
       "2238         0.217480         0.308880          0.151515  ...     0.0   \n",
       "2239         0.061992         0.007722          0.005051  ...     0.0   \n",
       "\n",
       "      Together  Widow  Basic  Graduation  Master  PhD  Year_joined  \\\n",
       "0          0.0    0.0    0.0         1.0     0.0  0.0          0.0   \n",
       "1          0.0    0.0    0.0         1.0     0.0  0.0          1.0   \n",
       "2          1.0    0.0    0.0         1.0     0.0  0.0          0.5   \n",
       "3          1.0    0.0    0.0         1.0     0.0  0.0          1.0   \n",
       "4          0.0    0.0    0.0         0.0     0.0  1.0          1.0   \n",
       "...        ...    ...    ...         ...     ...  ...          ...   \n",
       "2235       0.0    0.0    0.0         1.0     0.0  0.0          0.5   \n",
       "2236       1.0    0.0    0.0         0.0     0.0  1.0          1.0   \n",
       "2237       0.0    0.0    0.0         1.0     0.0  0.0          1.0   \n",
       "2238       1.0    0.0    0.0         0.0     1.0  0.0          1.0   \n",
       "2239       0.0    0.0    0.0         0.0     0.0  1.0          0.0   \n",
       "\n",
       "      Month_joined  Day_joined  \n",
       "0         0.272727    0.266667  \n",
       "1         0.636364    0.066667  \n",
       "2         0.636364    0.666667  \n",
       "3         0.818182    0.033333  \n",
       "4         0.000000    0.600000  \n",
       "...            ...         ...  \n",
       "2235      0.454545    0.400000  \n",
       "2236      0.818182    0.166667  \n",
       "2237      0.000000    0.800000  \n",
       "2238      0.000000    0.766667  \n",
       "2239      0.818182    0.466667  \n",
       "\n",
       "[2192 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dat = array(dfram)\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(dfram)\n",
    "\n",
    "dfram[list(dfram.columns.values)] = scaler.fit_transform(dfram[list(dfram.columns.values)])\n",
    "\n",
    "dfram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f8a68",
   "metadata": {},
   "source": [
    "## Initial Fast Trial to train MLP to see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8674d10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9315459212778094\n",
      "Test Accuracy:  0.8861047835990888\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE Train:  0.06845407872219053\n",
      "MSE Test:  0.11389521640091116\n",
      "\n",
      "\n",
      " Precision and Recall: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Response       0.93      0.94      0.93       378\n",
      "    Responds       0.59      0.57      0.58        61\n",
      "\n",
      "    accuracy                           0.89       439\n",
      "   macro avg       0.76      0.76      0.76       439\n",
      "weighted avg       0.88      0.89      0.89       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "train, test = train_test_split(dfram, test_size = 0.2, random_state = 21)\n",
    "#feature selection\n",
    "X_train, X_test = train[[ 'Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n",
    "       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
    "       'NumWebVisitsMonth','AcceptedCmp3', 'AcceptedCmp5',\n",
    "       'Year_joined', 'Month_joined', 'Day_joined','Recency']], test[[ 'Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n",
    "       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
    "       'NumWebVisitsMonth','AcceptedCmp3',  'AcceptedCmp5',\n",
    "       'Year_joined', 'Month_joined', 'Day_joined','Recency']]\n",
    "\n",
    "# X_train, X_test = train.drop(columns = ['Response']), test.drop(columns = ['Response'])\n",
    "y_train, y_test = train['Response'], test['Response']\n",
    "\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(15,15,),activation=\"relu\",solver = 'sgd', learning_rate_init = 0.3, max_iter = 500, random_state=1)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Train Accuracy: \", clf.score(X_train, y_train))\n",
    "print(\"Test Accuracy: \", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\n",
    "plt.xticks(ticks=np.arange(2) + 0.5, labels=['No Response','Responds'])\n",
    "plt.yticks(ticks=np.arange(2) + 0.5, labels=['No Response','Responds'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMSE Train: \", mean_squared_error(array(y_train), clf.predict(X_train)))\n",
    "print(\"MSE Test: \", mean_squared_error(array(y_test), y_pred))\n",
    "\n",
    "print(\"\\n\\n Precision and Recall: \")\n",
    "print(classification_report(y_test, y_pred, target_names = ['No Response','Responds']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9846a",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873f13d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,cross_validate\n",
    "\n",
    "#Using all original datasets for cross validation \n",
    "# data = dfram.drop(columns = ['Response'])\n",
    "# target = dfram['Response']\n",
    "\n",
    "CV = cross_validate(clf, X_train, y_train, cv = 10, scoring = ['accuracy','neg_mean_squared_error'], error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21f6811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Per Cross Validation: \n",
      "[0.83522727 0.875      0.82386364 0.85142857 0.85142857 0.82285714\n",
      " 0.81142857 0.88       0.85714286 0.84      ]\n",
      "\n",
      "Avg Accuracy:  0.8448376623376623\n",
      "\n",
      "MSE Per Cross Validation: \n",
      "[0.16477273 0.125      0.17613636 0.14857143 0.14857143 0.17714286\n",
      " 0.18857143 0.12       0.14285714 0.16      ]\n",
      "\n",
      "Avg MSE:  0.15516233766233764\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Per Cross Validation: \")\n",
    "print(CV['test_accuracy'])\n",
    "\n",
    "ta = CV['test_accuracy']\n",
    "print(\"\\nAvg Accuracy: \", sum(ta)/10)\n",
    "\n",
    "print(\"\\nMSE Per Cross Validation: \")\n",
    "print(abs(CV['test_neg_mean_squared_error']))\n",
    "\n",
    "mse = abs(CV['test_neg_mean_squared_error'])\n",
    "print(\"\\nAvg MSE: \", sum(mse)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee92052",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aadd21e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 14:31:30,400]\u001b[0m A new study created in memory with name: no-name-092030dd-718d-4497-8174-b745962e8ce8\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:34,178]\u001b[0m Trial 0 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 119.35780005022963, 'learning_rate_init': 0.0050070264837249615, 'n_layers': 2, 'n_units_0': 53, 'n_units_1': 64}. Best is trial 0 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:34,877]\u001b[0m Trial 1 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.004987580034576274, 'learning_rate_init': 0.0087216124004492, 'n_layers': 3, 'n_units_0': 41, 'n_units_1': 77, 'n_units_2': 41}. Best is trial 0 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:38,421]\u001b[0m Trial 2 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 15.359586513666386, 'learning_rate_init': 0.00507315539168967, 'n_layers': 3, 'n_units_0': 49, 'n_units_1': 80, 'n_units_2': 13}. Best is trial 0 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:38,877]\u001b[0m Trial 3 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 6.941831347240727, 'learning_rate_init': 0.11522285243717972, 'n_layers': 3, 'n_units_0': 36, 'n_units_1': 57, 'n_units_2': 56}. Best is trial 0 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:40,777]\u001b[0m Trial 4 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 7.652298349894237, 'learning_rate_init': 0.03056667378130227, 'n_layers': 3, 'n_units_0': 97, 'n_units_1': 3, 'n_units_2': 31}. Best is trial 0 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:41,778]\u001b[0m Trial 5 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 82.48658536643264, 'learning_rate_init': 0.0168584991231831, 'n_layers': 2, 'n_units_0': 82, 'n_units_1': 86}. Best is trial 0 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:42,003]\u001b[0m Trial 6 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 13.218012342540131, 'learning_rate_init': 0.3384319821793268, 'n_layers': 2, 'n_units_0': 14, 'n_units_1': 48}. Best is trial 0 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:52,439]\u001b[0m Trial 7 finished with value: 0.89749430523918 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0012721582172805878, 'learning_rate_init': 0.008830334660725381, 'n_layers': 3, 'n_units_0': 79, 'n_units_1': 64, 'n_units_2': 67}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:52,720]\u001b[0m Trial 8 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 999.8316970142977, 'learning_rate_init': 0.016917955506198405, 'n_layers': 2, 'n_units_0': 14, 'n_units_1': 42}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:31:55,106]\u001b[0m Trial 9 finished with value: 0.8792710706150342 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.09476961563255518, 'learning_rate_init': 0.03369026456030875, 'n_layers': 3, 'n_units_0': 47, 'n_units_1': 74, 'n_units_2': 63}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:32:03,925]\u001b[0m Trial 10 finished with value: 0.8815489749430524 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0002654908893229858, 'learning_rate_init': 0.0012049328289900278, 'n_layers': 3, 'n_units_0': 70, 'n_units_1': 26, 'n_units_2': 96}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:32:16,054]\u001b[0m Trial 11 finished with value: 0.8861047835990888 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00016058915216412805, 'learning_rate_init': 0.0011679885491281262, 'n_layers': 3, 'n_units_0': 78, 'n_units_1': 25, 'n_units_2': 91}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:32:32,452]\u001b[0m Trial 12 finished with value: 0.89749430523918 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00015404002651715561, 'learning_rate_init': 0.0011227454598444363, 'n_layers': 3, 'n_units_0': 72, 'n_units_1': 100, 'n_units_2': 90}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:32:47,365]\u001b[0m Trial 13 finished with value: 0.8792710706150342 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.004729531200061704, 'learning_rate_init': 0.002503808507808155, 'n_layers': 3, 'n_units_0': 65, 'n_units_1': 98, 'n_units_2': 74}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:32:56,498]\u001b[0m Trial 14 finished with value: 0.8861047835990888 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0023830458044649828, 'learning_rate_init': 0.002549769639185615, 'n_layers': 3, 'n_units_0': 100, 'n_units_1': 97, 'n_units_2': 81}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:32:59,292]\u001b[0m Trial 15 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.09201382482954691, 'learning_rate_init': 0.0758105222800147, 'n_layers': 3, 'n_units_0': 87, 'n_units_1': 35, 'n_units_2': 73}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:33:13,162]\u001b[0m Trial 16 finished with value: 0.8883826879271071 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0005775066092890932, 'learning_rate_init': 0.002657367411087851, 'n_layers': 3, 'n_units_0': 64, 'n_units_1': 63, 'n_units_2': 100}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:16,957]\u001b[0m Trial 17 finished with value: 0.876993166287016 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.029003335809034657, 'learning_rate_init': 0.008452410162406544, 'n_layers': 2, 'n_units_0': 29, 'n_units_1': 89}. Best is trial 7 with value: 0.89749430523918.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:18,516]\u001b[0m Trial 18 finished with value: 0.9020501138952164 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6549135006192921, 'learning_rate_init': 0.06894158626951814, 'n_layers': 3, 'n_units_0': 58, 'n_units_1': 68, 'n_units_2': 80}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:19,893]\u001b[0m Trial 19 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.18531386666307048, 'learning_rate_init': 0.44151438726728703, 'n_layers': 3, 'n_units_0': 57, 'n_units_1': 4, 'n_units_2': 86}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:21,386]\u001b[0m Trial 20 finished with value: 0.8952164009111617 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.9474186535541766, 'learning_rate_init': 0.1015445705339479, 'n_layers': 2, 'n_units_0': 32, 'n_units_1': 100}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:23,277]\u001b[0m Trial 21 finished with value: 0.8883826879271071 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7261604611683699, 'learning_rate_init': 0.045014651336502426, 'n_layers': 3, 'n_units_0': 74, 'n_units_1': 69, 'n_units_2': 67}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:24,179]\u001b[0m Trial 22 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0001094532037392353, 'learning_rate_init': 0.06296199532204691, 'n_layers': 3, 'n_units_0': 85, 'n_units_1': 56, 'n_units_2': 80}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 14:33:26,493]\u001b[0m Trial 23 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.001108544547770742, 'learning_rate_init': 0.21543778488406784, 'n_layers': 3, 'n_units_0': 61, 'n_units_1': 87, 'n_units_2': 49}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:30,401]\u001b[0m Trial 24 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.021134440241775576, 'learning_rate_init': 0.14535430956459025, 'n_layers': 3, 'n_units_0': 88, 'n_units_1': 46, 'n_units_2': 63}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:44,424]\u001b[0m Trial 25 finished with value: 0.8929384965831435 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0141433745124119, 'learning_rate_init': 0.01221450607649564, 'n_layers': 3, 'n_units_0': 73, 'n_units_1': 66, 'n_units_2': 84}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:45,201]\u001b[0m Trial 26 finished with value: 0.8906605922551253 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.2489925580028223, 'learning_rate_init': 0.02350675192341472, 'n_layers': 3, 'n_units_0': 68, 'n_units_1': 53, 'n_units_2': 72}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:33:52,573]\u001b[0m Trial 27 finished with value: 0.8792710706150342 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0005054815980271737, 'learning_rate_init': 0.004805888516235535, 'n_layers': 3, 'n_units_0': 1, 'n_units_1': 72, 'n_units_2': 91}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:00,578]\u001b[0m Trial 28 finished with value: 0.8815489749430524 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.001629938069148683, 'learning_rate_init': 0.059226388767877294, 'n_layers': 3, 'n_units_0': 93, 'n_units_1': 80, 'n_units_2': 50}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:34:08,887]\u001b[0m Trial 29 finished with value: 0.8997722095671982 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 2.511259154182654, 'learning_rate_init': 0.0017863766909665183, 'n_layers': 2, 'n_units_0': 53, 'n_units_1': 63}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:17,315]\u001b[0m Trial 30 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 2.475504784329484, 'learning_rate_init': 0.004468833277951055, 'n_layers': 2, 'n_units_0': 54, 'n_units_1': 59}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:34:24,968]\u001b[0m Trial 31 finished with value: 0.9020501138952164 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 2.811954709437028, 'learning_rate_init': 0.0017838285048632284, 'n_layers': 2, 'n_units_0': 44, 'n_units_1': 66}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:27,589]\u001b[0m Trial 32 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 42.98870076908819, 'learning_rate_init': 0.001814144776011494, 'n_layers': 2, 'n_units_0': 40, 'n_units_1': 39}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:33,923]\u001b[0m Trial 33 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.3919143580028883, 'learning_rate_init': 0.0017865347712683229, 'n_layers': 2, 'n_units_0': 48, 'n_units_1': 62}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:38,836]\u001b[0m Trial 34 finished with value: 0.8997722095671982 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 2.803115443772878, 'learning_rate_init': 0.008171073991791138, 'n_layers': 2, 'n_units_0': 44, 'n_units_1': 68}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:47,976]\u001b[0m Trial 35 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 3.7434405883249178, 'learning_rate_init': 0.003567036156816495, 'n_layers': 2, 'n_units_0': 43, 'n_units_1': 81}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:48,883]\u001b[0m Trial 36 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 26.514233591298545, 'learning_rate_init': 0.006786026891022619, 'n_layers': 2, 'n_units_0': 57, 'n_units_1': 71}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:49,786]\u001b[0m Trial 37 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 242.40890379781337, 'learning_rate_init': 0.00356133310219797, 'n_layers': 2, 'n_units_0': 28, 'n_units_1': 52}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:51,226]\u001b[0m Trial 38 finished with value: 0.8883826879271071 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 3.885606703376386, 'learning_rate_init': 0.001895075674538386, 'n_layers': 2, 'n_units_0': 23, 'n_units_1': 78}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:34:59,959]\u001b[0m Trial 39 finished with value: 0.8838268792710706 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.4544079088803551, 'learning_rate_init': 0.01421838502836002, 'n_layers': 2, 'n_units_0': 41, 'n_units_1': 58}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:35:00,857]\u001b[0m Trial 40 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 13.109419326522222, 'learning_rate_init': 0.025644731427905, 'n_layers': 2, 'n_units_0': 52, 'n_units_1': 67}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:35:08,390]\u001b[0m Trial 41 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 1.7936056163408023, 'learning_rate_init': 0.006552877281464853, 'n_layers': 2, 'n_units_0': 36, 'n_units_1': 75}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:35:10,022]\u001b[0m Trial 42 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 7.368030848424399, 'learning_rate_init': 0.009679348704541466, 'n_layers': 2, 'n_units_0': 46, 'n_units_1': 63}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:35:18,022]\u001b[0m Trial 43 finished with value: 0.8861047835990888 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.1829416829497943, 'learning_rate_init': 0.018335691407528318, 'n_layers': 2, 'n_units_0': 55, 'n_units_1': 70}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:35:23,347]\u001b[0m Trial 44 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.07804292195582414, 'learning_rate_init': 0.03678953280909253, 'n_layers': 2, 'n_units_0': 61, 'n_units_1': 83}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:35:24,198]\u001b[0m Trial 45 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 147.35375228999345, 'learning_rate_init': 0.00689388577492534, 'n_layers': 2, 'n_units_0': 78, 'n_units_1': 50}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:35:30,032]\u001b[0m Trial 46 finished with value: 0.8883826879271071 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 4.544526012890904, 'learning_rate_init': 0.003472457395352478, 'n_layers': 2, 'n_units_0': 35, 'n_units_1': 75}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 14:35:35,170]\u001b[0m Trial 47 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 19.744233085355255, 'learning_rate_init': 0.0014271160559466518, 'n_layers': 2, 'n_units_0': 45, 'n_units_1': 91}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:35:43,545]\u001b[0m Trial 48 finished with value: 0.8952164009111617 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.7641542872917811, 'learning_rate_init': 0.011258393622129978, 'n_layers': 2, 'n_units_0': 50, 'n_units_1': 61}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:35:44,865]\u001b[0m Trial 49 finished with value: 0.8906605922551253 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 2.0583326282599708, 'learning_rate_init': 0.00103097043456657, 'n_layers': 2, 'n_units_0': 24, 'n_units_1': 46}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:35:54,500]\u001b[0m Trial 50 finished with value: 0.8861047835990888 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.1610698973286073, 'learning_rate_init': 0.0024026574362980553, 'n_layers': 3, 'n_units_0': 60, 'n_units_1': 56, 'n_units_2': 2}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:36:09,624]\u001b[0m Trial 51 finished with value: 0.8838268792710706 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00023898571132052236, 'learning_rate_init': 0.001693348144213144, 'n_layers': 3, 'n_units_0': 65, 'n_units_1': 94, 'n_units_2': 59}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:36:19,954]\u001b[0m Trial 52 finished with value: 0.8792710706150342 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00470359775222011, 'learning_rate_init': 0.0013099847642522855, 'n_layers': 3, 'n_units_0': 78, 'n_units_1': 10, 'n_units_2': 76}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:36:32,536]\u001b[0m Trial 53 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.04992793309183176, 'learning_rate_init': 0.0010452388622179253, 'n_layers': 3, 'n_units_0': 69, 'n_units_1': 67, 'n_units_2': 91}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:36:41,581]\u001b[0m Trial 54 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 9.86784220508912, 'learning_rate_init': 0.0023487846031289483, 'n_layers': 3, 'n_units_0': 82, 'n_units_1': 84, 'n_units_2': 69}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:36:50,018]\u001b[0m Trial 55 finished with value: 0.876993166287016 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0005910366299543158, 'learning_rate_init': 0.16911889864036042, 'n_layers': 3, 'n_units_0': 72, 'n_units_1': 29, 'n_units_2': 30}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:36:51,826]\u001b[0m Trial 56 finished with value: 0.8952164009111617 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.302323766903851, 'learning_rate_init': 0.0029610252516555015, 'n_layers': 3, 'n_units_0': 38, 'n_units_1': 77, 'n_units_2': 41}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:37:10,786]\u001b[0m Trial 57 finished with value: 0.8815489749430524 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.2728800964889934, 'learning_rate_init': 0.0014097129030026018, 'n_layers': 3, 'n_units_0': 92, 'n_units_1': 64, 'n_units_2': 84}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:17,887]\u001b[0m Trial 58 finished with value: 0.8952164009111617 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0001012870794244948, 'learning_rate_init': 0.005565889816662223, 'n_layers': 3, 'n_units_0': 51, 'n_units_1': 72, 'n_units_2': 42}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:18,405]\u001b[0m Trial 59 finished with value: 0.8952164009111617 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.6257031101573612, 'learning_rate_init': 0.08574068923423345, 'n_layers': 3, 'n_units_0': 82, 'n_units_1': 55, 'n_units_2': 57}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:18,698]\u001b[0m Trial 60 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 52.086145871995214, 'learning_rate_init': 0.25689850626820115, 'n_layers': 2, 'n_units_0': 66, 'n_units_1': 19}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:19,847]\u001b[0m Trial 61 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.5451106099648717, 'learning_rate_init': 0.0026884078099356067, 'n_layers': 3, 'n_units_0': 42, 'n_units_1': 77, 'n_units_2': 31}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:27,360]\u001b[0m Trial 62 finished with value: 0.8906605922551253 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.7923327320592454, 'learning_rate_init': 0.012403228222935084, 'n_layers': 2, 'n_units_0': 49, 'n_units_1': 59}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:28,413]\u001b[0m Trial 63 finished with value: 0.8906605922551253 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.5071395627827897, 'learning_rate_init': 0.11064413622817967, 'n_layers': 3, 'n_units_0': 83, 'n_units_1': 54, 'n_units_2': 55}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:29,707]\u001b[0m Trial 64 finished with value: 0.8678815489749431 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.011210483297374485, 'learning_rate_init': 0.0468635979499919, 'n_layers': 3, 'n_units_0': 77, 'n_units_1': 41, 'n_units_2': 65}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:30,221]\u001b[0m Trial 65 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 5.6562953525179935, 'learning_rate_init': 0.10065383970460726, 'n_layers': 2, 'n_units_0': 34, 'n_units_1': 95}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:34,786]\u001b[0m Trial 66 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 2.6939930171061994, 'learning_rate_init': 0.009373716832102566, 'n_layers': 2, 'n_units_0': 57, 'n_units_1': 61}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:36,803]\u001b[0m Trial 67 finished with value: 0.8906605922551253 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.00981901022546, 'learning_rate_init': 0.003060392978291024, 'n_layers': 3, 'n_units_0': 39, 'n_units_1': 67, 'n_units_2': 38}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:38,250]\u001b[0m Trial 68 finished with value: 0.89749430523918 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0003550913312244366, 'learning_rate_init': 0.06521399073615429, 'n_layers': 2, 'n_units_0': 62, 'n_units_1': 91}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 14:37:39,575]\u001b[0m Trial 69 finished with value: 0.8952164009111617 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0001874766127753385, 'learning_rate_init': 0.06494627456079335, 'n_layers': 2, 'n_units_0': 62, 'n_units_1': 100}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:42,136]\u001b[0m Trial 70 finished with value: 0.8861047835990888 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0004511298962577753, 'learning_rate_init': 0.018979842064268365, 'n_layers': 2, 'n_units_0': 54, 'n_units_1': 90}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:47,798]\u001b[0m Trial 71 finished with value: 0.8883826879271071 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.00012648788616339315, 'learning_rate_init': 0.004234899897359753, 'n_layers': 3, 'n_units_0': 52, 'n_units_1': 73, 'n_units_2': 44}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:51,229]\u001b[0m Trial 72 finished with value: 0.8701594533029613 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00027857620251185624, 'learning_rate_init': 0.055896269549866345, 'n_layers': 2, 'n_units_0': 61, 'n_units_1': 97}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:53,276]\u001b[0m Trial 73 finished with value: 0.876993166287016 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0014728031466298389, 'learning_rate_init': 0.08384739376797434, 'n_layers': 2, 'n_units_0': 32, 'n_units_1': 100}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:55,239]\u001b[0m Trial 74 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0008786195121827211, 'learning_rate_init': 0.13219214490710068, 'n_layers': 2, 'n_units_0': 18, 'n_units_1': 94}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:56,114]\u001b[0m Trial 75 finished with value: 0.89749430523918 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 2.78047638956375, 'learning_rate_init': 0.17614638284070305, 'n_layers': 2, 'n_units_0': 45, 'n_units_1': 88}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:37:56,725]\u001b[0m Trial 76 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0027878091623152724, 'learning_rate_init': 0.4353723570086529, 'n_layers': 2, 'n_units_0': 45, 'n_units_1': 84}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:02,474]\u001b[0m Trial 77 finished with value: 0.8724373576309795 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.000345503744304817, 'learning_rate_init': 0.028154424500821867, 'n_layers': 2, 'n_units_0': 75, 'n_units_1': 88}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:04,158]\u001b[0m Trial 78 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 2.5814224183456296, 'learning_rate_init': 0.18725103767319257, 'n_layers': 2, 'n_units_0': 71, 'n_units_1': 81}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:04,839]\u001b[0m Trial 79 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 11.841488487666679, 'learning_rate_init': 0.30028654137382865, 'n_layers': 2, 'n_units_0': 57, 'n_units_1': 69}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:11,218]\u001b[0m Trial 80 finished with value: 0.8861047835990888 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.34885245951525273, 'learning_rate_init': 0.03678451801553641, 'n_layers': 2, 'n_units_0': 67, 'n_units_1': 92}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:13,345]\u001b[0m Trial 81 finished with value: 0.8883826879271071 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.6992735092237095, 'learning_rate_init': 0.002138075207152193, 'n_layers': 3, 'n_units_0': 44, 'n_units_1': 65, 'n_units_2': 22}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:15,859]\u001b[0m Trial 82 finished with value: 0.8997722095671982 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 3.505489677770527, 'learning_rate_init': 0.0015734200231655269, 'n_layers': 3, 'n_units_0': 38, 'n_units_1': 86, 'n_units_2': 78}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:18,499]\u001b[0m Trial 83 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 3.765698726985804, 'learning_rate_init': 0.0015697128489088766, 'n_layers': 3, 'n_units_0': 48, 'n_units_1': 87, 'n_units_2': 78}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:20,357]\u001b[0m Trial 84 finished with value: 0.8633257403189066 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 7.159479129870936, 'learning_rate_init': 0.0019852300482990416, 'n_layers': 3, 'n_units_0': 47, 'n_units_1': 85, 'n_units_2': 95}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:23,435]\u001b[0m Trial 85 finished with value: 0.8952164009111617 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 3.2439015492185144, 'learning_rate_init': 0.0011307132739769305, 'n_layers': 2, 'n_units_0': 38, 'n_units_1': 97}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:29,066]\u001b[0m Trial 86 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 22.431473629211453, 'learning_rate_init': 0.0013228003533737617, 'n_layers': 3, 'n_units_0': 63, 'n_units_1': 80, 'n_units_2': 100}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:30,846]\u001b[0m Trial 87 finished with value: 0.8906605922551253 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 4.701725307168014, 'learning_rate_init': 0.04459918147999834, 'n_layers': 2, 'n_units_0': 42, 'n_units_1': 90}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:36,639]\u001b[0m Trial 88 finished with value: 0.8883826879271071 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00018201807045510425, 'learning_rate_init': 0.001560404034765175, 'n_layers': 2, 'n_units_0': 59, 'n_units_1': 93}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:37,601]\u001b[0m Trial 89 finished with value: 0.8587699316628702 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.000835215924443148, 'learning_rate_init': 0.021336709758750922, 'n_layers': 3, 'n_units_0': 29, 'n_units_1': 50, 'n_units_2': 91}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:38:46,741]\u001b[0m Trial 90 finished with value: 0.8861047835990888 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.2654252975714831, 'learning_rate_init': 0.00811341024510033, 'n_layers': 2, 'n_units_0': 54, 'n_units_1': 76}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:48,283]\u001b[0m Trial 91 finished with value: 0.8792710706150342 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00014873462109816108, 'learning_rate_init': 0.06949524291857527, 'n_layers': 2, 'n_units_0': 64, 'n_units_1': 100}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:49,659]\u001b[0m Trial 92 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00023632025179567308, 'learning_rate_init': 0.14808930144592614, 'n_layers': 2, 'n_units_0': 50, 'n_units_1': 98}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:51,937]\u001b[0m Trial 93 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 9.405828380210522, 'learning_rate_init': 0.014740018421277862, 'n_layers': 2, 'n_units_0': 69, 'n_units_1': 96}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 14:38:52,584]\u001b[0m Trial 94 finished with value: 0.8929384965831435 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 1.269035327370073, 'learning_rate_init': 0.09813395778412241, 'n_layers': 2, 'n_units_0': 34, 'n_units_1': 61}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:38:55,003]\u001b[0m Trial 95 finished with value: 0.89749430523918 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 2.7423928908452124, 'learning_rate_init': 0.0010889367993165452, 'n_layers': 2, 'n_units_0': 40, 'n_units_1': 69}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:39:05,927]\u001b[0m Trial 96 finished with value: 0.8701594533029613 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 5.7689570125894996, 'learning_rate_init': 0.0010071657548605803, 'n_layers': 3, 'n_units_0': 36, 'n_units_1': 71, 'n_units_2': 70}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:39:08,521]\u001b[0m Trial 97 finished with value: 0.8906605922551253 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.9523746273762617, 'learning_rate_init': 0.0012090869085936208, 'n_layers': 2, 'n_units_0': 40, 'n_units_1': 69}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 14:39:19,622]\u001b[0m Trial 98 finished with value: 0.8906605922551253 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 1.0965763415679148, 'learning_rate_init': 0.0015703963548728645, 'n_layers': 2, 'n_units_0': 89, 'n_units_1': 58}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 14:39:24,597]\u001b[0m Trial 99 finished with value: 0.8747152619589977 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.6015521833431245, 'learning_rate_init': 0.0021961799376557646, 'n_layers': 3, 'n_units_0': 44, 'n_units_1': 74, 'n_units_2': 88}. Best is trial 18 with value: 0.9020501138952164.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuning Parameters : {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6549135006192921, 'learning_rate_init': 0.06894158626951814, 'n_layers': 3, 'n_units_0': 58, 'n_units_1': 68, 'n_units_2': 80} \n",
      " with accuracy of : 0.90 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    activation = trial.suggest_categorical('activation',['logistic', 'tanh', 'relu'])\n",
    "    solver = trial.suggest_categorical('solver',['sgd', 'adam'])\n",
    "    alpha=trial.suggest_float(\"alpha\",1e-4,1e3,log=True)\n",
    "    learning_rate_init=trial.suggest_float(\"learning_rate_init\",0.001,0.5,log=True)\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 3)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f'n_units_{i}', 1, 100))\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=tuple(layers),activation=activation ,solver = solver, alpha=alpha, learning_rate_init = learning_rate_init, max_iter = 500, random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "trial=study.best_trial\n",
    "print(\"Best Tuning Parameters : {} \\n with accuracy of : {:.2f} %\".format(trial.params,trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a7f87f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "#optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4a54944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9418140330861381\n",
      "Test Accuracy:  0.9066059225512528\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE Train:  0.05818596691386195\n",
      "MSE Test:  0.09339407744874716\n",
      "\n",
      "\n",
      " Precision and Recall: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Response       0.93      0.96      0.95       378\n",
      "    Responds       0.70      0.57      0.63        61\n",
      "\n",
      "    accuracy                           0.91       439\n",
      "   macro avg       0.82      0.77      0.79       439\n",
      "weighted avg       0.90      0.91      0.90       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(75,43,),activation=\"relu\",solver = 'adam', alpha = 0.1003182605021015, learning_rate_init = 0.01351109170188156, max_iter = 500, random_state=1)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Train Accuracy: \", clf.score(X_train, y_train))\n",
    "print(\"Test Accuracy: \", clf.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\n",
    "plt.xticks(ticks=np.arange(2) + 0.5 , labels=['No Response','Responds'])\n",
    "plt.yticks(ticks=np.arange(2) + 0.5, labels=['No Response','Responds'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMSE Train: \", mean_squared_error(array(y_train), clf.predict(X_train)))\n",
    "print(\"MSE Test: \", mean_squared_error(array(y_test), y_pred))\n",
    "\n",
    "print(\"\\n\\n Precision and Recall: \")\n",
    "print(classification_report(y_test, y_pred, target_names = ['No Response','Responds']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64155d0",
   "metadata": {},
   "source": [
    "## Due to imbalanced class and our sample is not huge, trying Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d941321",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    1487\n",
      "1.0     266\n",
      "Name: Response, dtype: int64\n",
      "0.0    1487\n",
      "1.0    1487\n",
      "Name: Response, dtype: int64\n",
      "\n",
      "After Oversampling\n",
      "\n",
      "Test Accuracy:  0.8451025056947609\n",
      "MSE Test:  0.1548974943052392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Response       0.94      0.88      0.91       378\n",
      "    Responds       0.46      0.64      0.53        61\n",
      "\n",
      "    accuracy                           0.85       439\n",
      "   macro avg       0.70      0.76      0.72       439\n",
      "weighted avg       0.87      0.85      0.86       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "print(y_train.value_counts())\n",
    "\n",
    "ros = RandomOverSampler(random_state=21)\n",
    "X_os,y_os = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(y_os.value_counts())\n",
    "\n",
    "\n",
    "clf.fit(X_os, np.asarray(y_os))\n",
    "\n",
    "print('\\nAfter Oversampling\\n' )\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Accuracy: \", clf.score(X_test, y_test))\n",
    "print(\"MSE Test: \", mean_squared_error(array(y_test), y_pred))\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names = ['No Response','Responds']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5972f1a",
   "metadata": {},
   "source": [
    "## Hyperparam tuning for oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cba1c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 00:39:29,135]\u001b[0m A new study created in memory with name: no-name-9cc3e114-0304-4a12-8bc7-25238573455e\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:30,904]\u001b[0m Trial 0 finished with value: 0.806378132118451 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0014506849431726975, 'learning_rate_init': 0.04374473765840331, 'n_layers': 3, 'n_units_0': 18, 'n_units_1': 76, 'n_units_2': 22}. Best is trial 0 with value: 0.806378132118451.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:31,211]\u001b[0m Trial 1 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 377.8421537236102, 'learning_rate_init': 0.0013969041853617644, 'n_layers': 2, 'n_units_0': 17, 'n_units_1': 90}. Best is trial 0 with value: 0.806378132118451.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:32,470]\u001b[0m Trial 2 finished with value: 0.8451025056947609 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.0007121773990136708, 'learning_rate_init': 0.19265059524612968, 'n_layers': 2, 'n_units_0': 59, 'n_units_1': 81}. Best is trial 2 with value: 0.8451025056947609.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:32,785]\u001b[0m Trial 3 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 75.51642413839501, 'learning_rate_init': 0.005405626374663315, 'n_layers': 3, 'n_units_0': 100, 'n_units_1': 34, 'n_units_2': 20}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:33,163]\u001b[0m Trial 4 finished with value: 0.7220956719817767 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.7151405136044077, 'learning_rate_init': 0.34407851007948675, 'n_layers': 2, 'n_units_0': 71, 'n_units_1': 19}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:33,484]\u001b[0m Trial 5 finished with value: 0.7835990888382688 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 4.175009061939356, 'learning_rate_init': 0.16236845032602557, 'n_layers': 2, 'n_units_0': 72, 'n_units_1': 39}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:33,881]\u001b[0m Trial 6 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 52.2279075241746, 'learning_rate_init': 0.006063074559072068, 'n_layers': 3, 'n_units_0': 99, 'n_units_1': 3, 'n_units_2': 19}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:34,927]\u001b[0m Trial 7 finished with value: 0.826879271070615 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.05609710308765499, 'learning_rate_init': 0.025748218072159625, 'n_layers': 2, 'n_units_0': 15, 'n_units_1': 14}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:35,871]\u001b[0m Trial 8 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 10.43527540148554, 'learning_rate_init': 0.00595417910353689, 'n_layers': 2, 'n_units_0': 34, 'n_units_1': 41}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:36,212]\u001b[0m Trial 9 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 154.03360119768985, 'learning_rate_init': 0.06707903272431395, 'n_layers': 3, 'n_units_0': 36, 'n_units_1': 47, 'n_units_2': 69}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:41,239]\u001b[0m Trial 10 finished with value: 0.8883826879271071 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.10369260682662, 'learning_rate_init': 0.001106677908330008, 'n_layers': 3, 'n_units_0': 91, 'n_units_1': 65, 'n_units_2': 4}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:45,568]\u001b[0m Trial 11 finished with value: 0.8542141230068337 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.03607035956563589, 'learning_rate_init': 0.0011412997492164182, 'n_layers': 3, 'n_units_0': 99, 'n_units_1': 60, 'n_units_2': 2}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:47,016]\u001b[0m Trial 12 finished with value: 0.8587699316628702 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.017725334292531165, 'learning_rate_init': 0.004349551408540345, 'n_layers': 3, 'n_units_0': 83, 'n_units_1': 64, 'n_units_2': 35}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:49,725]\u001b[0m Trial 13 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.5215851741501524, 'learning_rate_init': 0.002934570004178034, 'n_layers': 3, 'n_units_0': 84, 'n_units_1': 28, 'n_units_2': 4}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:50,152]\u001b[0m Trial 14 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 905.7374651569523, 'learning_rate_init': 0.011873421872815197, 'n_layers': 3, 'n_units_0': 87, 'n_units_1': 65, 'n_units_2': 51}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:50,926]\u001b[0m Trial 15 finished with value: 0.7835990888382688 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 7.071901618338508, 'learning_rate_init': 0.00215253950202244, 'n_layers': 3, 'n_units_0': 55, 'n_units_1': 98, 'n_units_2': 40}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:52,328]\u001b[0m Trial 16 finished with value: 0.8519362186788155 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.00017707564725502573, 'learning_rate_init': 0.010716676315191902, 'n_layers': 3, 'n_units_0': 71, 'n_units_1': 32, 'n_units_2': 91}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:56,288]\u001b[0m Trial 17 finished with value: 0.8701594533029613 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.39653283449473636, 'learning_rate_init': 0.002166676494475947, 'n_layers': 3, 'n_units_0': 84, 'n_units_1': 56, 'n_units_2': 4}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:58,956]\u001b[0m Trial 18 finished with value: 0.7494305239179955 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.0070786142177071375, 'learning_rate_init': 0.0011544047407115784, 'n_layers': 3, 'n_units_0': 40, 'n_units_1': 55, 'n_units_2': 4}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:39:59,604]\u001b[0m Trial 19 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.1096488669204572, 'learning_rate_init': 0.0023206058788838636, 'n_layers': 3, 'n_units_0': 1, 'n_units_1': 74, 'n_units_2': 59}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:01,627]\u001b[0m Trial 20 finished with value: 0.8428246013667426 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.3579592824290903, 'learning_rate_init': 0.011785098043956384, 'n_layers': 3, 'n_units_0': 64, 'n_units_1': 50, 'n_units_2': 29}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:02,168]\u001b[0m Trial 21 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 37.959461545531305, 'learning_rate_init': 0.003295237515153387, 'n_layers': 3, 'n_units_0': 91, 'n_units_1': 69, 'n_units_2': 15}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:05,495]\u001b[0m Trial 22 finished with value: 0.8656036446469249 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.34644073210143633, 'learning_rate_init': 0.0022988585567890033, 'n_layers': 3, 'n_units_0': 81, 'n_units_1': 25, 'n_units_2': 2}. Best is trial 10 with value: 0.8883826879271071.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:09,728]\u001b[0m Trial 23 finished with value: 0.8997722095671982 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.25236590704988343, 'learning_rate_init': 0.0019971696930916796, 'n_layers': 3, 'n_units_0': 77, 'n_units_1': 54, 'n_units_2': 11}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:13,142]\u001b[0m Trial 24 finished with value: 0.8861047835990888 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.1661552955395859, 'learning_rate_init': 0.00163269519362463, 'n_layers': 3, 'n_units_0': 77, 'n_units_1': 51, 'n_units_2': 14}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 00:40:14,728]\u001b[0m Trial 25 finished with value: 0.8086560364464692 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.006895836861235392, 'learning_rate_init': 0.0011043075213910097, 'n_layers': 3, 'n_units_0': 75, 'n_units_1': 46, 'n_units_2': 13}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:17,616]\u001b[0m Trial 26 finished with value: 0.8678815489749431 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.17399013342616207, 'learning_rate_init': 0.001715523591041265, 'n_layers': 3, 'n_units_0': 50, 'n_units_1': 54, 'n_units_2': 31}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:21,712]\u001b[0m Trial 27 finished with value: 0.8132118451025057 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 2.459991031197081, 'learning_rate_init': 0.007881040528061545, 'n_layers': 3, 'n_units_0': 93, 'n_units_1': 79, 'n_units_2': 12}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:23,502]\u001b[0m Trial 28 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.013877394564951198, 'learning_rate_init': 0.0038620809967751303, 'n_layers': 3, 'n_units_0': 65, 'n_units_1': 87, 'n_units_2': 43}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:26,843]\u001b[0m Trial 29 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.001243723804398968, 'learning_rate_init': 0.024571771427536786, 'n_layers': 3, 'n_units_0': 47, 'n_units_1': 69, 'n_units_2': 26}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:27,909]\u001b[0m Trial 30 finished with value: 0.8428246013667426 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.003278391503621079, 'learning_rate_init': 0.0436845241157863, 'n_layers': 3, 'n_units_0': 76, 'n_units_1': 40, 'n_units_2': 81}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:31,012]\u001b[0m Trial 31 finished with value: 0.8792710706150342 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.10898991622694704, 'learning_rate_init': 0.001596919491302009, 'n_layers': 3, 'n_units_0': 79, 'n_units_1': 60, 'n_units_2': 10}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:33,360]\u001b[0m Trial 32 finished with value: 0.8314350797266514 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.14341761373205655, 'learning_rate_init': 0.0016017844583212043, 'n_layers': 3, 'n_units_0': 92, 'n_units_1': 60, 'n_units_2': 13}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:37,026]\u001b[0m Trial 33 finished with value: 0.89749430523918 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.04860890938101245, 'learning_rate_init': 0.0016273630772079414, 'n_layers': 3, 'n_units_0': 65, 'n_units_1': 72, 'n_units_2': 11}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:40,903]\u001b[0m Trial 34 finished with value: 0.8906605922551253 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.024097896828391027, 'learning_rate_init': 0.001039978572815604, 'n_layers': 3, 'n_units_0': 63, 'n_units_1': 75, 'n_units_2': 23}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "/Users/roy/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "\n",
      "\u001b[32m[I 2021-12-02 00:40:47,791]\u001b[0m Trial 35 finished with value: 0.8086560364464692 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.03800112880898883, 'learning_rate_init': 0.0010922858256908095, 'n_layers': 2, 'n_units_0': 64, 'n_units_1': 85}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:49,077]\u001b[0m Trial 36 finished with value: 0.8337129840546698 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.9891527711515214, 'learning_rate_init': 0.003181010455246116, 'n_layers': 3, 'n_units_0': 57, 'n_units_1': 74, 'n_units_2': 24}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:51,643]\u001b[0m Trial 37 finished with value: 0.8724373576309795 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.014977526417462767, 'learning_rate_init': 0.004697890670457384, 'n_layers': 2, 'n_units_0': 68, 'n_units_1': 91}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:54,597]\u001b[0m Trial 38 finished with value: 0.8018223234624146 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.003477860067828726, 'learning_rate_init': 0.0010542408033414722, 'n_layers': 3, 'n_units_0': 58, 'n_units_1': 70, 'n_units_2': 20}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:54,921]\u001b[0m Trial 39 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.00017356367848247913, 'learning_rate_init': 0.3808046876073619, 'n_layers': 3, 'n_units_0': 46, 'n_units_1': 81, 'n_units_2': 9}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:55,277]\u001b[0m Trial 40 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0005327119932126162, 'learning_rate_init': 0.12664858968637985, 'n_layers': 2, 'n_units_0': 61, 'n_units_1': 93}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:40:58,733]\u001b[0m Trial 41 finished with value: 0.8678815489749431 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.048247804699029895, 'learning_rate_init': 0.0016945300098248186, 'n_layers': 3, 'n_units_0': 77, 'n_units_1': 65, 'n_units_2': 18}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:02,238]\u001b[0m Trial 42 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.21961416681570706, 'learning_rate_init': 0.0014917569023126175, 'n_layers': 3, 'n_units_0': 69, 'n_units_1': 46, 'n_units_2': 9}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:05,360]\u001b[0m Trial 43 finished with value: 0.8542141230068337 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.06335798981038575, 'learning_rate_init': 0.002587056190864356, 'n_layers': 3, 'n_units_0': 88, 'n_units_1': 76, 'n_units_2': 23}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:07,848]\u001b[0m Trial 44 finished with value: 0.8952164009111617 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.029580041858564853, 'learning_rate_init': 0.0018442190970124814, 'n_layers': 3, 'n_units_0': 96, 'n_units_1': 51, 'n_units_2': 18}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:11,046]\u001b[0m Trial 45 finished with value: 0.8496583143507973 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.023707155479194244, 'learning_rate_init': 0.007896240255317306, 'n_layers': 3, 'n_units_0': 93, 'n_units_1': 60, 'n_units_2': 32}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:14,725]\u001b[0m Trial 46 finished with value: 0.8792710706150342 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.007909483329196821, 'learning_rate_init': 0.001312407706061186, 'n_layers': 3, 'n_units_0': 100, 'n_units_1': 70, 'n_units_2': 19}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:17,185]\u001b[0m Trial 47 finished with value: 0.8633257403189066 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.0030734427456008038, 'learning_rate_init': 0.0019900351667749474, 'n_layers': 3, 'n_units_0': 96, 'n_units_1': 84, 'n_units_2': 7}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:19,024]\u001b[0m Trial 48 finished with value: 0.8815489749430524 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.08036344262312327, 'learning_rate_init': 0.003740208006686971, 'n_layers': 3, 'n_units_0': 87, 'n_units_1': 65, 'n_units_2': 37}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 00:41:22,955]\u001b[0m Trial 49 finished with value: 0.8246013667425968 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.6950778210576388, 'learning_rate_init': 0.016315576751978693, 'n_layers': 3, 'n_units_0': 26, 'n_units_1': 51, 'n_units_2': 27}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:25,551]\u001b[0m Trial 50 finished with value: 0.806378132118451 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.026407753422705866, 'learning_rate_init': 0.0026936208992277464, 'n_layers': 2, 'n_units_0': 53, 'n_units_1': 3}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:28,995]\u001b[0m Trial 51 finished with value: 0.8542141230068337 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.8697444875500218, 'learning_rate_init': 0.0013587281841296235, 'n_layers': 3, 'n_units_0': 74, 'n_units_1': 35, 'n_units_2': 15}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:32,819]\u001b[0m Trial 52 finished with value: 0.8519362186788155 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.2233765628839367, 'learning_rate_init': 0.0016950777926397983, 'n_layers': 3, 'n_units_0': 70, 'n_units_1': 57, 'n_units_2': 18}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:34,616]\u001b[0m Trial 53 finished with value: 0.8291571753986332 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.48149528768317007, 'learning_rate_init': 0.005035149131032832, 'n_layers': 3, 'n_units_0': 81, 'n_units_1': 44, 'n_units_2': 2}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:38,229]\u001b[0m Trial 54 finished with value: 0.8678815489749431 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.07152998969599768, 'learning_rate_init': 0.0019183651087084756, 'n_layers': 3, 'n_units_0': 84, 'n_units_1': 51, 'n_units_2': 7}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:41,852]\u001b[0m Trial 55 finished with value: 0.8656036446469249 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.012718790140149346, 'learning_rate_init': 0.0013053911163546374, 'n_layers': 3, 'n_units_0': 88, 'n_units_1': 77, 'n_units_2': 49}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:45,176]\u001b[0m Trial 56 finished with value: 0.876993166287016 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.038522031367571916, 'learning_rate_init': 0.001038607232073886, 'n_layers': 3, 'n_units_0': 96, 'n_units_1': 62, 'n_units_2': 22}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:45,587]\u001b[0m Trial 57 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 16.131981620007387, 'learning_rate_init': 0.002959699262819298, 'n_layers': 3, 'n_units_0': 61, 'n_units_1': 11, 'n_units_2': 16}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:46,409]\u001b[0m Trial 58 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.2799276597866438, 'learning_rate_init': 0.002193428380043819, 'n_layers': 3, 'n_units_0': 67, 'n_units_1': 54, 'n_units_2': 1}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:47,616]\u001b[0m Trial 59 finished with value: 0.8451025056947609 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.09998814169182962, 'learning_rate_init': 0.034866715635430034, 'n_layers': 3, 'n_units_0': 73, 'n_units_1': 73, 'n_units_2': 7}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:49,246]\u001b[0m Trial 60 finished with value: 0.8792710706150342 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.008538326014409205, 'learning_rate_init': 0.006075926369654508, 'n_layers': 3, 'n_units_0': 79, 'n_units_1': 50, 'n_units_2': 12}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:50,854]\u001b[0m Trial 61 finished with value: 0.8815489749430524 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.07276422246152438, 'learning_rate_init': 0.00365595341093612, 'n_layers': 3, 'n_units_0': 87, 'n_units_1': 65, 'n_units_2': 41}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:54,470]\u001b[0m Trial 62 finished with value: 0.8382687927107062 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.02534073912291713, 'learning_rate_init': 0.0013444980083328834, 'n_layers': 3, 'n_units_0': 90, 'n_units_1': 67, 'n_units_2': 52}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:41:55,939]\u001b[0m Trial 63 finished with value: 0.8246013667425968 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 3.821569410945764, 'learning_rate_init': 0.0024347365542523335, 'n_layers': 3, 'n_units_0': 96, 'n_units_1': 58, 'n_units_2': 33}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:00,169]\u001b[0m Trial 64 finished with value: 0.8952164009111617 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.14447988433888623, 'learning_rate_init': 0.001000741943663874, 'n_layers': 3, 'n_units_0': 85, 'n_units_1': 37, 'n_units_2': 44}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:04,321]\u001b[0m Trial 65 finished with value: 0.8724373576309795 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.7816161835486088, 'learning_rate_init': 0.0010122674637200492, 'n_layers': 3, 'n_units_0': 80, 'n_units_1': 37, 'n_units_2': 97}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:06,463]\u001b[0m Trial 66 finished with value: 0.8086560364464692 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.15031541572362345, 'learning_rate_init': 0.001945849418146079, 'n_layers': 3, 'n_units_0': 84, 'n_units_1': 29, 'n_units_2': 70}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:07,683]\u001b[0m Trial 67 finished with value: 0.8747152619589977 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.37467787582743906, 'learning_rate_init': 0.07088059328244709, 'n_layers': 3, 'n_units_0': 75, 'n_units_1': 19, 'n_units_2': 4}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:11,674]\u001b[0m Trial 68 finished with value: 0.8815489749430524 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.14419675114699032, 'learning_rate_init': 0.0013094255887460523, 'n_layers': 3, 'n_units_0': 95, 'n_units_1': 42, 'n_units_2': 60}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:12,716]\u001b[0m Trial 69 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.039861095887149874, 'learning_rate_init': 0.2805796304482663, 'n_layers': 3, 'n_units_0': 72, 'n_units_1': 49, 'n_units_2': 28}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:16,394]\u001b[0m Trial 70 finished with value: 0.8701594533029613 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.019540376068041338, 'learning_rate_init': 0.0018059101887755425, 'n_layers': 3, 'n_units_0': 62, 'n_units_1': 54, 'n_units_2': 15}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:19,226]\u001b[0m Trial 71 finished with value: 0.8656036446469249 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.08062849765559704, 'learning_rate_init': 0.003779220928620549, 'n_layers': 3, 'n_units_0': 86, 'n_units_1': 63, 'n_units_2': 38}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:21,568]\u001b[0m Trial 72 finished with value: 0.8678815489749431 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.05500617750485251, 'learning_rate_init': 0.0015162082776149443, 'n_layers': 3, 'n_units_0': 89, 'n_units_1': 73, 'n_units_2': 47}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:26,209]\u001b[0m Trial 73 finished with value: 0.8838268792710706 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.2642277027067715, 'learning_rate_init': 0.0012172390547451535, 'n_layers': 3, 'n_units_0': 82, 'n_units_1': 44, 'n_units_2': 56}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 00:42:28,860]\u001b[0m Trial 74 finished with value: 0.876993166287016 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.160358349847602, 'learning_rate_init': 0.001199401596877843, 'n_layers': 3, 'n_units_0': 77, 'n_units_1': 43, 'n_units_2': 59}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:31,149]\u001b[0m Trial 75 finished with value: 0.8724373576309795 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.2754436058199541, 'learning_rate_init': 0.0014750205087653275, 'n_layers': 3, 'n_units_0': 83, 'n_units_1': 31, 'n_units_2': 62}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:32,731]\u001b[0m Trial 76 finished with value: 0.7995444191343963 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.48156095557297607, 'learning_rate_init': 0.001017750413556579, 'n_layers': 3, 'n_units_0': 4, 'n_units_1': 48, 'n_units_2': 66}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:35,020]\u001b[0m Trial 77 finished with value: 0.8451025056947609 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.1785234933152515, 'learning_rate_init': 0.002319779327345453, 'n_layers': 3, 'n_units_0': 66, 'n_units_1': 37, 'n_units_2': 11}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:35,309]\u001b[0m Trial 78 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.11201527970037357, 'learning_rate_init': 0.001870677799554891, 'n_layers': 3, 'n_units_0': 92, 'n_units_1': 45, 'n_units_2': 45}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:39,191]\u001b[0m Trial 79 finished with value: 0.8747152619589977 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.01095817930949545, 'learning_rate_init': 0.0012229098100817284, 'n_layers': 3, 'n_units_0': 99, 'n_units_1': 25, 'n_units_2': 22}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:41,023]\u001b[0m Trial 80 finished with value: 0.8678815489749431 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.004617590263786583, 'learning_rate_init': 0.002819227970949034, 'n_layers': 3, 'n_units_0': 78, 'n_units_1': 39, 'n_units_2': 55}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:44,229]\u001b[0m Trial 81 finished with value: 0.8542141230068337 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.03158167026133118, 'learning_rate_init': 0.0014978917957410164, 'n_layers': 3, 'n_units_0': 86, 'n_units_1': 80, 'n_units_2': 40}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:47,964]\u001b[0m Trial 82 finished with value: 0.8792710706150342 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.1380708728777862, 'learning_rate_init': 0.0011951925063274824, 'n_layers': 3, 'n_units_0': 94, 'n_units_1': 40, 'n_units_2': 78}. Best is trial 23 with value: 0.8997722095671982.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:51,232]\u001b[0m Trial 83 finished with value: 0.9066059225512528 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.060638234900149024, 'learning_rate_init': 0.002086421193586337, 'n_layers': 3, 'n_units_0': 82, 'n_units_1': 67, 'n_units_2': 55}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:54,292]\u001b[0m Trial 84 finished with value: 0.9020501138952164 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.05150999196479176, 'learning_rate_init': 0.0020982072696219124, 'n_layers': 3, 'n_units_0': 82, 'n_units_1': 71, 'n_units_2': 54}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:56,692]\u001b[0m Trial 85 finished with value: 0.8861047835990888 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.05512033193897383, 'learning_rate_init': 0.002126742387824207, 'n_layers': 3, 'n_units_0': 71, 'n_units_1': 72, 'n_units_2': 54}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:42:59,272]\u001b[0m Trial 86 finished with value: 0.8861047835990888 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.018400084558658582, 'learning_rate_init': 0.003132110189922319, 'n_layers': 3, 'n_units_0': 90, 'n_units_1': 68, 'n_units_2': 6}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:07,971]\u001b[0m Trial 87 finished with value: 0.8701594533029613 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.02028658493594985, 'learning_rate_init': 0.0032659515456265526, 'n_layers': 3, 'n_units_0': 91, 'n_units_1': 78, 'n_units_2': 6}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:13,409]\u001b[0m Trial 88 finished with value: 0.8496583143507973 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.6591492684456224, 'learning_rate_init': 0.001608231746381735, 'n_layers': 3, 'n_units_0': 56, 'n_units_1': 83, 'n_units_2': 25}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:17,310]\u001b[0m Trial 89 finished with value: 0.8473804100227791 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.03245396712674428, 'learning_rate_init': 0.0023980601728078262, 'n_layers': 3, 'n_units_0': 71, 'n_units_1': 71, 'n_units_2': 56}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "/Users/roy/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "\n",
      "\u001b[32m[I 2021-12-02 00:43:25,352]\u001b[0m Trial 90 finished with value: 0.8086560364464692 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 0.09960838027795998, 'learning_rate_init': 0.0018140368841983582, 'n_layers': 2, 'n_units_0': 52, 'n_units_1': 67}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:29,826]\u001b[0m Trial 91 finished with value: 0.8405466970387244 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.050595065687968815, 'learning_rate_init': 0.002183269178572075, 'n_layers': 3, 'n_units_0': 69, 'n_units_1': 75, 'n_units_2': 53}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:33,706]\u001b[0m Trial 92 finished with value: 0.8633257403189066 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.0532258594753368, 'learning_rate_init': 0.0020240496011244705, 'n_layers': 3, 'n_units_0': 76, 'n_units_1': 62, 'n_units_2': 49}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:35,915]\u001b[0m Trial 93 finished with value: 0.8223234624145785 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.19902925099658683, 'learning_rate_init': 0.0026644539589994313, 'n_layers': 3, 'n_units_0': 80, 'n_units_1': 72, 'n_units_2': 66}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:40,869]\u001b[0m Trial 94 finished with value: 0.8587699316628702 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.014666589544661324, 'learning_rate_init': 0.001419011876173043, 'n_layers': 3, 'n_units_0': 85, 'n_units_1': 68, 'n_units_2': 9}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:43,126]\u001b[0m Trial 95 finished with value: 0.826879271070615 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.00582487299472411, 'learning_rate_init': 0.004605309634544806, 'n_layers': 3, 'n_units_0': 63, 'n_units_1': 67, 'n_units_2': 17}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:45,348]\u001b[0m Trial 96 finished with value: 0.7722095671981777 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.011571578725930152, 'learning_rate_init': 0.01830177690592245, 'n_layers': 3, 'n_units_0': 90, 'n_units_1': 58, 'n_units_2': 13}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 00:43:50,096]\u001b[0m Trial 97 finished with value: 0.8929384965831435 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.03944405885549392, 'learning_rate_init': 0.0016214168975634072, 'n_layers': 3, 'n_units_0': 72, 'n_units_1': 53, 'n_units_2': 45}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 00:43:53,484]\u001b[0m Trial 98 finished with value: 0.8792710706150342 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.0015143709460735076, 'learning_rate_init': 0.0016815502837194927, 'n_layers': 3, 'n_units_0': 97, 'n_units_1': 53, 'n_units_2': 6}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n",
      "/Users/roy/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "\n",
      "\u001b[32m[I 2021-12-02 00:44:03,869]\u001b[0m Trial 99 finished with value: 0.8496583143507973 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.08577849388022843, 'learning_rate_init': 0.0011491152262522162, 'n_layers': 3, 'n_units_0': 59, 'n_units_1': 52, 'n_units_2': 20}. Best is trial 83 with value: 0.9066059225512528.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuning Parameters : {'activation': 'relu', 'solver': 'adam', 'alpha': 0.060638234900149024, 'learning_rate_init': 0.002086421193586337, 'n_layers': 3, 'n_units_0': 82, 'n_units_1': 67, 'n_units_2': 55} \n",
      " with accuracy of : 0.91 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    activation = trial.suggest_categorical('activation',['logistic', 'tanh', 'relu'])\n",
    "    solver = trial.suggest_categorical('solver',['sgd', 'adam'])\n",
    "    alpha=trial.suggest_float(\"alpha\",1e-4,1e3,log=True)\n",
    "    learning_rate_init=trial.suggest_float(\"learning_rate_init\",0.001,0.5,log=True)\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 3)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f'n_units_{i}', 1, 100))\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=tuple(layers),activation=activation ,solver = solver, alpha=alpha, learning_rate_init = learning_rate_init, max_iter = 500, random_state=1)\n",
    "    clf.fit(X_os, np.asarray(y_os))\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "trial=study.best_trial\n",
    "print(\"Best Tuning Parameters : {} \\n with accuracy of : {:.2f} %\".format(trial.params,trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1c716",
   "metadata": {},
   "source": [
    "## Oversampling hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89978e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9903023388476897\n",
      "Test Accuracy:  0.9066059225512528\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9066059225512528\n",
      "MSE Test:  0.09339407744874716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Response       0.94      0.95      0.95       378\n",
      "    Responds       0.68      0.62      0.65        61\n",
      "\n",
      "    accuracy                           0.91       439\n",
      "   macro avg       0.81      0.79      0.80       439\n",
      "weighted avg       0.90      0.91      0.90       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(82,67,55,),activation=\"relu\",solver = 'adam', alpha = 0.060638234900149024, learning_rate_init = 0.002086421193586337, max_iter = 500, random_state=1)\n",
    "\n",
    "\n",
    "clf.fit(X_os, np.asarray(y_os))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Train Accuracy: \", clf.score(X_train, y_train))\n",
    "print(\"Test Accuracy: \", clf.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\n",
    "plt.xticks(ticks=np.arange(2) + 0.5 , labels=['No Response','Responds'])\n",
    "plt.yticks(ticks=np.arange(2) + 0.5, labels=['No Response','Responds'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Test Accuracy: \", clf.score(X_test, y_test))\n",
    "print(\"MSE Test: \", mean_squared_error(array(y_test), y_pred))\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names = ['No Response','Responds']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444df01c",
   "metadata": {},
   "source": [
    "## Trying Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6f645f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    1487\n",
      "1.0     266\n",
      "Name: Response, dtype: int64\n",
      "0.0    266\n",
      "1.0    266\n",
      "Name: Response, dtype: int64\n",
      "\n",
      "After Undersampling\n",
      "\n",
      "Test Accuracy:  0.8633257403189066\n",
      "MSE Test:  0.1366742596810934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Response       0.93      0.90      0.92       378\n",
      "    Responds       0.51      0.61      0.55        61\n",
      "\n",
      "    accuracy                           0.86       439\n",
      "   macro avg       0.72      0.76      0.74       439\n",
      "weighted avg       0.88      0.86      0.87       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "print(y_train.value_counts())\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(75,43,),activation=\"relu\",solver = 'adam', alpha = 0.1003182605021015, learning_rate_init = 0.01351109170188156, max_iter = 500, random_state=1)\n",
    "\n",
    "rus = RandomUnderSampler(random_state=21)\n",
    "X_us,y_us = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(y_us.value_counts())\n",
    "\n",
    "clf.fit(X_us, np.asarray(y_us))\n",
    "\n",
    "print('\\nAfter Undersampling\\n' )\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Accuracy: \", clf.score(X_test, y_test))\n",
    "print(\"MSE Test: \", mean_squared_error(array(y_test), y_pred))\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names = ['No Response','Responds']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04a565",
   "metadata": {},
   "source": [
    "## Hyperparam tuning for undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc398e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 18:15:57,383]\u001b[0m A new study created in memory with name: no-name-87259cce-2cc6-4462-8954-dbd16175977d\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:15:57,851]\u001b[0m Trial 0 finished with value: 0.7289293849658315 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 1.5509078087580659, 'learning_rate_init': 0.037732501017779294, 'n_layers': 2, 'n_units_0': 9, 'n_units_1': 67}. Best is trial 0 with value: 0.7289293849658315.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:15:58,505]\u001b[0m Trial 1 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 744.8836031993811, 'learning_rate_init': 0.018850645157086763, 'n_layers': 3, 'n_units_0': 4, 'n_units_1': 96, 'n_units_2': 100}. Best is trial 0 with value: 0.7289293849658315.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:15:59,388]\u001b[0m Trial 2 finished with value: 0.7927107061503417 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0004683046413508604, 'learning_rate_init': 0.012839108074031014, 'n_layers': 2, 'n_units_0': 27, 'n_units_1': 7}. Best is trial 2 with value: 0.7927107061503417.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:15:59,687]\u001b[0m Trial 3 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00025620057678329413, 'learning_rate_init': 0.14842417324716334, 'n_layers': 2, 'n_units_0': 20, 'n_units_1': 73}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:00,239]\u001b[0m Trial 4 finished with value: 0.7790432801822323 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.8373434773772729, 'learning_rate_init': 0.01804360199587399, 'n_layers': 2, 'n_units_0': 81, 'n_units_1': 30}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:00,462]\u001b[0m Trial 5 finished with value: 0.14578587699316628 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.09837123933600284, 'learning_rate_init': 0.0012799256724401122, 'n_layers': 2, 'n_units_0': 93, 'n_units_1': 69}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:00,858]\u001b[0m Trial 6 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 1.4064935163594756, 'learning_rate_init': 0.062452461443901805, 'n_layers': 3, 'n_units_0': 57, 'n_units_1': 46, 'n_units_2': 46}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 18:16:02,653]\u001b[0m Trial 7 finished with value: 0.7562642369020501 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 11.835647980417077, 'learning_rate_init': 0.0015187004303983942, 'n_layers': 2, 'n_units_0': 17, 'n_units_1': 47}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 18:16:11,992]\u001b[0m Trial 8 finished with value: 0.7813211845102506 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.03180803210836355, 'learning_rate_init': 0.001447568400499295, 'n_layers': 3, 'n_units_0': 46, 'n_units_1': 60, 'n_units_2': 53}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:12,171]\u001b[0m Trial 9 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 78.66008735974667, 'learning_rate_init': 0.03180052298079981, 'n_layers': 2, 'n_units_0': 71, 'n_units_1': 52}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:13,452]\u001b[0m Trial 10 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00035207906222044845, 'learning_rate_init': 0.32331359984992836, 'n_layers': 3, 'n_units_0': 38, 'n_units_1': 95, 'n_units_2': 5}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:13,998]\u001b[0m Trial 11 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00011958731499697069, 'learning_rate_init': 0.48948167710936136, 'n_layers': 3, 'n_units_0': 36, 'n_units_1': 100, 'n_units_2': 1}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:15,489]\u001b[0m Trial 12 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.003020418432193026, 'learning_rate_init': 0.3047906015682717, 'n_layers': 3, 'n_units_0': 36, 'n_units_1': 83, 'n_units_2': 7}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:15,847]\u001b[0m Trial 13 finished with value: 0.7175398633257403 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.003961513016958226, 'learning_rate_init': 0.13856142227948848, 'n_layers': 3, 'n_units_0': 20, 'n_units_1': 83, 'n_units_2': 31}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:16,173]\u001b[0m Trial 14 finished with value: 0.6902050113895216 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0023345841953296555, 'learning_rate_init': 0.16122517542278897, 'n_layers': 3, 'n_units_0': 53, 'n_units_1': 85, 'n_units_2': 80}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:17,307]\u001b[0m Trial 15 finished with value: 0.7403189066059226 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.00011545891056677856, 'learning_rate_init': 0.10467335578021215, 'n_layers': 2, 'n_units_0': 33, 'n_units_1': 75}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:17,462]\u001b[0m Trial 16 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.01602849500565682, 'learning_rate_init': 0.30106281461197976, 'n_layers': 2, 'n_units_0': 46, 'n_units_1': 92}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:17,624]\u001b[0m Trial 17 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.02704437464446307, 'learning_rate_init': 0.44223975731880355, 'n_layers': 3, 'n_units_0': 67, 'n_units_1': 99, 'n_units_2': 26}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:20,190]\u001b[0m Trial 18 finished with value: 0.7972665148063781 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.13374938862974664, 'learning_rate_init': 0.004826360681284056, 'n_layers': 3, 'n_units_0': 66, 'n_units_1': 98, 'n_units_2': 25}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:20,603]\u001b[0m Trial 19 finished with value: 0.7722095671981777 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.01137824916977583, 'learning_rate_init': 0.07217963044947893, 'n_layers': 2, 'n_units_0': 19, 'n_units_1': 30}. Best is trial 3 with value: 0.8610478359908884.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:20,791]\u001b[0m Trial 20 finished with value: 0.8929384965831435 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0006595041584502584, 'learning_rate_init': 0.18913808210499217, 'n_layers': 2, 'n_units_0': 87, 'n_units_1': 76}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:21,231]\u001b[0m Trial 21 finished with value: 0.42369020501138954 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.023787597725583108, 'learning_rate_init': 0.2755685776303193, 'n_layers': 2, 'n_units_0': 94, 'n_units_1': 88}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:21,462]\u001b[0m Trial 22 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0006585287451879448, 'learning_rate_init': 0.19315623587722427, 'n_layers': 2, 'n_units_0': 100, 'n_units_1': 75}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:23,088]\u001b[0m Trial 23 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0017584466688322499, 'learning_rate_init': 0.4673759237722642, 'n_layers': 3, 'n_units_0': 81, 'n_units_1': 99, 'n_units_2': 23}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 18:16:24,502]\u001b[0m Trial 24 finished with value: 0.7585421412300684 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0001594084872825783, 'learning_rate_init': 0.08241799171676349, 'n_layers': 2, 'n_units_0': 31, 'n_units_1': 59}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:24,756]\u001b[0m Trial 25 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0006898485014832261, 'learning_rate_init': 0.21078753400773192, 'n_layers': 2, 'n_units_0': 100, 'n_units_1': 75}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "/home/matthew/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-12-04 18:16:28,985]\u001b[0m Trial 26 finished with value: 0.7972665148063781 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0102609859772861, 'learning_rate_init': 0.008039388786999018, 'n_layers': 2, 'n_units_0': 71, 'n_units_1': 91}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:30,439]\u001b[0m Trial 27 finished with value: 0.7107061503416856 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0008962183092743685, 'learning_rate_init': 0.0469307636724389, 'n_layers': 2, 'n_units_0': 88, 'n_units_1': 78}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:30,592]\u001b[0m Trial 28 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.058747420196173995, 'learning_rate_init': 0.4953685969080842, 'n_layers': 3, 'n_units_0': 61, 'n_units_1': 91, 'n_units_2': 59}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:30,885]\u001b[0m Trial 29 finished with value: 0.8382687927107062 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.2880229197226332, 'learning_rate_init': 0.28984184455203776, 'n_layers': 2, 'n_units_0': 46, 'n_units_1': 66}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:31,396]\u001b[0m Trial 30 finished with value: 0.7289293849658315 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 0.005814615173206104, 'learning_rate_init': 0.11015657316525719, 'n_layers': 2, 'n_units_0': 78, 'n_units_1': 37}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:32,905]\u001b[0m Trial 31 finished with value: 0.6902050113895216 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.013374605293124696, 'learning_rate_init': 0.19245458779449764, 'n_layers': 2, 'n_units_0': 9, 'n_units_1': 80}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:33,738]\u001b[0m Trial 32 finished with value: 0.7403189066059226 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0002894969446441823, 'learning_rate_init': 0.1339940500757824, 'n_layers': 2, 'n_units_0': 43, 'n_units_1': 69}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:35,243]\u001b[0m Trial 33 finished with value: 0.724373576309795 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.001148082408845648, 'learning_rate_init': 0.043351715818541685, 'n_layers': 2, 'n_units_0': 100, 'n_units_1': 63}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:35,384]\u001b[0m Trial 34 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0003825253663138843, 'learning_rate_init': 0.22280558565160258, 'n_layers': 2, 'n_units_0': 1, 'n_units_1': 73}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:35,683]\u001b[0m Trial 35 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 6.437500698157438, 'learning_rate_init': 0.3252036184052311, 'n_layers': 2, 'n_units_0': 75, 'n_units_1': 9}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:36,261]\u001b[0m Trial 36 finished with value: 0.724373576309795 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.04557976751373784, 'learning_rate_init': 0.02738304603957123, 'n_layers': 2, 'n_units_0': 85, 'n_units_1': 92}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:38,012]\u001b[0m Trial 37 finished with value: 0.7289293849658315 and parameters: {'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.26522704803277153, 'learning_rate_init': 0.09854618167567417, 'n_layers': 2, 'n_units_0': 62, 'n_units_1': 86}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:38,254]\u001b[0m Trial 38 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'adam', 'alpha': 34.814931034621274, 'learning_rate_init': 0.057538251304726985, 'n_layers': 2, 'n_units_0': 74, 'n_units_1': 8}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:38,688]\u001b[0m Trial 39 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 6.832820752300491, 'learning_rate_init': 0.4036298495126957, 'n_layers': 3, 'n_units_0': 89, 'n_units_1': 1, 'n_units_2': 67}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:38,900]\u001b[0m Trial 40 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 7.056615916412384, 'learning_rate_init': 0.015709377396052344, 'n_layers': 2, 'n_units_0': 92, 'n_units_1': 1}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:39,773]\u001b[0m Trial 41 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.842214626303828, 'learning_rate_init': 0.4186154991846438, 'n_layers': 3, 'n_units_0': 26, 'n_units_1': 100, 'n_units_2': 71}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:39,937]\u001b[0m Trial 42 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.11428293617362173, 'learning_rate_init': 0.34990087119906893, 'n_layers': 3, 'n_units_0': 57, 'n_units_1': 94, 'n_units_2': 13}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:40,182]\u001b[0m Trial 43 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.1324925420215298, 'learning_rate_init': 0.33704790983783484, 'n_layers': 3, 'n_units_0': 53, 'n_units_1': 94, 'n_units_2': 14}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:40,506]\u001b[0m Trial 44 finished with value: 0.6469248291571754 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.006041501991735912, 'learning_rate_init': 0.21915876607205964, 'n_layers': 3, 'n_units_0': 67, 'n_units_1': 89, 'n_units_2': 38}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:40,712]\u001b[0m Trial 45 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0005368237062912844, 'learning_rate_init': 0.1814413052566943, 'n_layers': 3, 'n_units_0': 41, 'n_units_1': 55, 'n_units_2': 19}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:41,171]\u001b[0m Trial 46 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 3.9694155412592362, 'learning_rate_init': 0.23425115810643293, 'n_layers': 3, 'n_units_0': 87, 'n_units_1': 16, 'n_units_2': 70}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:41,336]\u001b[0m Trial 47 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 283.3105199135028, 'learning_rate_init': 0.4163856122192219, 'n_layers': 3, 'n_units_0': 49, 'n_units_1': 26, 'n_units_2': 37}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:41,611]\u001b[0m Trial 48 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 872.8504212572936, 'learning_rate_init': 0.0024834720029006424, 'n_layers': 2, 'n_units_0': 49, 'n_units_1': 38}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 18:16:42,146]\u001b[0m Trial 49 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 257.4196434222991, 'learning_rate_init': 0.002389311206367655, 'n_layers': 3, 'n_units_0': 42, 'n_units_1': 54, 'n_units_2': 20}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:42,343]\u001b[0m Trial 50 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 50.24938826382917, 'learning_rate_init': 0.12834729577985585, 'n_layers': 3, 'n_units_0': 12, 'n_units_1': 20, 'n_units_2': 42}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:43,084]\u001b[0m Trial 51 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0001706603374492939, 'learning_rate_init': 0.1633938792564562, 'n_layers': 3, 'n_units_0': 37, 'n_units_1': 39, 'n_units_2': 2}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:43,274]\u001b[0m Trial 52 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 403.1355660018747, 'learning_rate_init': 0.003249397824924642, 'n_layers': 2, 'n_units_0': 24, 'n_units_1': 40}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:44,036]\u001b[0m Trial 53 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 968.5508531500154, 'learning_rate_init': 0.008978177090754248, 'n_layers': 2, 'n_units_0': 50, 'n_units_1': 29}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:44,683]\u001b[0m Trial 54 finished with value: 0.6150341685649203 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00021193902709623792, 'learning_rate_init': 0.15575477780274602, 'n_layers': 2, 'n_units_0': 94, 'n_units_1': 8}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:45,662]\u001b[0m Trial 55 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 1.8058776059671386, 'learning_rate_init': 0.3790424406078125, 'n_layers': 3, 'n_units_0': 76, 'n_units_1': 1, 'n_units_2': 87}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:46,388]\u001b[0m Trial 56 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 1.889572379235374, 'learning_rate_init': 0.2672686278406578, 'n_layers': 2, 'n_units_0': 77, 'n_units_1': 13}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:46,738]\u001b[0m Trial 57 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 1.8969243316686675, 'learning_rate_init': 0.27257531740743707, 'n_layers': 2, 'n_units_0': 83, 'n_units_1': 19}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:47,424]\u001b[0m Trial 58 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0014147815496850869, 'learning_rate_init': 0.17582556436973582, 'n_layers': 3, 'n_units_0': 30, 'n_units_1': 47, 'n_units_2': 14}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:47,973]\u001b[0m Trial 59 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 20.512938331616425, 'learning_rate_init': 0.0891019677102363, 'n_layers': 2, 'n_units_0': 97, 'n_units_1': 72}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:48,699]\u001b[0m Trial 60 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 18.356058934893632, 'learning_rate_init': 0.0829692326025627, 'n_layers': 2, 'n_units_0': 97, 'n_units_1': 79}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:49,229]\u001b[0m Trial 61 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 10.321764255398671, 'learning_rate_init': 0.3430426096758671, 'n_layers': 2, 'n_units_0': 96, 'n_units_1': 80}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:50,353]\u001b[0m Trial 62 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 4.837407202056964, 'learning_rate_init': 0.3278573921877581, 'n_layers': 3, 'n_units_0': 89, 'n_units_1': 84, 'n_units_2': 58}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:50,727]\u001b[0m Trial 63 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 5.347223071954178, 'learning_rate_init': 0.34166657504901526, 'n_layers': 3, 'n_units_0': 90, 'n_units_1': 4, 'n_units_2': 62}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:51,842]\u001b[0m Trial 64 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 3.068202505078911, 'learning_rate_init': 0.23889525517669483, 'n_layers': 3, 'n_units_0': 91, 'n_units_1': 4, 'n_units_2': 66}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:52,032]\u001b[0m Trial 65 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.000594661525831064, 'learning_rate_init': 0.4090277341808396, 'n_layers': 3, 'n_units_0': 56, 'n_units_1': 55, 'n_units_2': 34}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:52,430]\u001b[0m Trial 66 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.0006874845404903612, 'learning_rate_init': 0.4961636529237733, 'n_layers': 3, 'n_units_0': 57, 'n_units_1': 57, 'n_units_2': 35}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:52,604]\u001b[0m Trial 67 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 382.95077051339285, 'learning_rate_init': 0.43322085301878016, 'n_layers': 3, 'n_units_0': 48, 'n_units_1': 25, 'n_units_2': 14}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:55,687]\u001b[0m Trial 68 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 151.77945299230268, 'learning_rate_init': 0.001554204411785424, 'n_layers': 3, 'n_units_0': 40, 'n_units_1': 35, 'n_units_2': 100}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:57,389]\u001b[0m Trial 69 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 165.14890218830647, 'learning_rate_init': 0.0010416204187958787, 'n_layers': 3, 'n_units_0': 41, 'n_units_1': 35, 'n_units_2': 91}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:57,884]\u001b[0m Trial 70 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 21.810789723271284, 'learning_rate_init': 0.06047457774684606, 'n_layers': 2, 'n_units_0': 96, 'n_units_1': 79}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:58,907]\u001b[0m Trial 71 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.6287286461533819, 'learning_rate_init': 0.3807730051298215, 'n_layers': 3, 'n_units_0': 36, 'n_units_1': 42, 'n_units_2': 82}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:59,208]\u001b[0m Trial 72 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 720.1470243723788, 'learning_rate_init': 0.06555889347082361, 'n_layers': 3, 'n_units_0': 52, 'n_units_1': 45, 'n_units_2': 8}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:59,509]\u001b[0m Trial 73 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 72.10111523280474, 'learning_rate_init': 0.17458057350183462, 'n_layers': 3, 'n_units_0': 54, 'n_units_1': 50, 'n_units_2': 8}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 18:16:59,663]\u001b[0m Trial 74 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 521.7502347376746, 'learning_rate_init': 0.12092997490092629, 'n_layers': 3, 'n_units_0': 53, 'n_units_1': 50, 'n_units_2': 1}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:16:59,958]\u001b[0m Trial 75 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 10.64600091743978, 'learning_rate_init': 0.08816065970898476, 'n_layers': 2, 'n_units_0': 78, 'n_units_1': 13}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:00,371]\u001b[0m Trial 76 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 16.227714558434837, 'learning_rate_init': 0.3473720827483858, 'n_layers': 2, 'n_units_0': 97, 'n_units_1': 82}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:01,515]\u001b[0m Trial 77 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 121.15495480114711, 'learning_rate_init': 0.0014430914119790532, 'n_layers': 2, 'n_units_0': 81, 'n_units_1': 77}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:01,882]\u001b[0m Trial 78 finished with value: 0.7562642369020501 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.4452325147740927, 'learning_rate_init': 0.0373822689226636, 'n_layers': 2, 'n_units_0': 34, 'n_units_1': 64}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:02,176]\u001b[0m Trial 79 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 1.5217368627470018, 'learning_rate_init': 0.26282936735358275, 'n_layers': 3, 'n_units_0': 44, 'n_units_1': 11, 'n_units_2': 53}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:02,345]\u001b[0m Trial 80 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00010517196739724404, 'learning_rate_init': 0.1438448544768838, 'n_layers': 2, 'n_units_0': 75, 'n_units_1': 23}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:02,574]\u001b[0m Trial 81 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.002247156150236612, 'learning_rate_init': 0.4912201040759382, 'n_layers': 2, 'n_units_0': 98, 'n_units_1': 69}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:02,791]\u001b[0m Trial 82 finished with value: 0.13895216400911162 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 37.61602225773608, 'learning_rate_init': 0.09336932470780336, 'n_layers': 2, 'n_units_0': 60, 'n_units_1': 58}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:04,977]\u001b[0m Trial 83 finished with value: 0.7995444191343963 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.0007915636172774838, 'learning_rate_init': 0.07554606260529469, 'n_layers': 2, 'n_units_0': 64, 'n_units_1': 28}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:05,455]\u001b[0m Trial 84 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.00044777117871041465, 'learning_rate_init': 0.022943666988845094, 'n_layers': 3, 'n_units_0': 90, 'n_units_1': 82, 'n_units_2': 47}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:06,655]\u001b[0m Trial 85 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0040130065294356, 'learning_rate_init': 0.2926462706853472, 'n_layers': 3, 'n_units_0': 86, 'n_units_1': 87, 'n_units_2': 60}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:06,842]\u001b[0m Trial 86 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00022053262879085078, 'learning_rate_init': 0.19956610103554284, 'n_layers': 3, 'n_units_0': 47, 'n_units_1': 32, 'n_units_2': 19}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:07,162]\u001b[0m Trial 87 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.00293448694608455, 'learning_rate_init': 0.3220762508042859, 'n_layers': 3, 'n_units_0': 86, 'n_units_1': 89, 'n_units_2': 60}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:07,513]\u001b[0m Trial 88 finished with value: 0.13895216400911162 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00011432609462652727, 'learning_rate_init': 0.14580638323448245, 'n_layers': 3, 'n_units_0': 38, 'n_units_1': 34, 'n_units_2': 99}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:08,022]\u001b[0m Trial 89 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.7961943193399214, 'learning_rate_init': 0.3888149175936679, 'n_layers': 3, 'n_units_0': 58, 'n_units_1': 44, 'n_units_2': 83}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:08,685]\u001b[0m Trial 90 finished with value: 0.8610478359908884 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.49185800691024967, 'learning_rate_init': 0.39721011491125663, 'n_layers': 3, 'n_units_0': 70, 'n_units_1': 42, 'n_units_2': 84}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:08,839]\u001b[0m Trial 91 finished with value: 0.8610478359908884 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 2.532665731523273, 'learning_rate_init': 0.24929321816684508, 'n_layers': 2, 'n_units_0': 79, 'n_units_1': 70}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:11,330]\u001b[0m Trial 92 finished with value: 0.7471526195899773 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.24076775763521813, 'learning_rate_init': 0.005591252689987081, 'n_layers': 3, 'n_units_0': 70, 'n_units_1': 40, 'n_units_2': 29}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:11,988]\u001b[0m Trial 93 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 2.9137861290729723, 'learning_rate_init': 0.48529089330582187, 'n_layers': 3, 'n_units_0': 55, 'n_units_1': 61, 'n_units_2': 76}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:12,173]\u001b[0m Trial 94 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'adam', 'alpha': 0.00018021027450062501, 'learning_rate_init': 0.19699932797784833, 'n_layers': 3, 'n_units_0': 35, 'n_units_1': 25, 'n_units_2': 11}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:12,419]\u001b[0m Trial 95 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 81.48896213245202, 'learning_rate_init': 0.05743363806542651, 'n_layers': 3, 'n_units_0': 39, 'n_units_1': 49, 'n_units_2': 34}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:12,901]\u001b[0m Trial 96 finished with value: 0.7038724373576309 and parameters: {'activation': 'tanh', 'solver': 'adam', 'alpha': 1.0489259353528393, 'learning_rate_init': 0.11398525312797393, 'n_layers': 2, 'n_units_0': 49, 'n_units_1': 72}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:13,400]\u001b[0m Trial 97 finished with value: 0.13895216400911162 and parameters: {'activation': 'logistic', 'solver': 'sgd', 'alpha': 760.5161450618639, 'learning_rate_init': 0.00271993170380317, 'n_layers': 3, 'n_units_0': 52, 'n_units_1': 45, 'n_units_2': 99}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n",
      "\u001b[32m[I 2021-12-04 18:17:15,536]\u001b[0m Trial 98 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 63.94279167890343, 'learning_rate_init': 0.0018748153221346672, 'n_layers': 2, 'n_units_0': 45, 'n_units_1': 48}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-04 18:17:15,885]\u001b[0m Trial 99 finished with value: 0.8610478359908884 and parameters: {'activation': 'relu', 'solver': 'sgd', 'alpha': 57.82016201847197, 'learning_rate_init': 0.014830054751445181, 'n_layers': 3, 'n_units_0': 44, 'n_units_1': 53, 'n_units_2': 94}. Best is trial 20 with value: 0.8929384965831435.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuning Parameters : {'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0006595041584502584, 'learning_rate_init': 0.18913808210499217, 'n_layers': 2, 'n_units_0': 87, 'n_units_1': 76} \n",
      " with accuracy of : 0.89 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    activation = trial.suggest_categorical('activation',['logistic', 'tanh', 'relu'])\n",
    "    solver = trial.suggest_categorical('solver',['sgd', 'adam'])\n",
    "    alpha=trial.suggest_float(\"alpha\",1e-4,1e3,log=True)\n",
    "    learning_rate_init=trial.suggest_float(\"learning_rate_init\",0.001,0.5,log=True)\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 3)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f'n_units_{i}', 1, 100))\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=tuple(layers),activation=activation ,solver = solver, alpha=alpha, learning_rate_init = learning_rate_init, max_iter = 500, random_state=1)\n",
    "    clf.fit(X_us, np.asarray(y_us))\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "trial=study.best_trial\n",
    "print(\"Best Tuning Parameters : {} \\n with accuracy of : {:.2f} %\".format(trial.params,trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc9064",
   "metadata": {},
   "source": [
    "## Checking hyperparam undersampling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "64ef248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.6766917293233082\n",
      "Test Accuracy:  0.8883826879271071\n",
      "MSE Test:  0.11161731207289294\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Response       0.92      0.95      0.94       378\n",
      "    Responds       0.62      0.49      0.55        61\n",
      "\n",
      "    accuracy                           0.89       439\n",
      "   macro avg       0.77      0.72      0.74       439\n",
      "weighted avg       0.88      0.89      0.88       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,99,),activation=\"relu\",solver = 'adam', alpha = 7.772046851402521, learning_rate_init = 0.15808530487217196, max_iter = 500, random_state=1)\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(90,28,13,67),activation=\"tanh\",solver = 'adam', alpha = 0.0023187496276111676, learning_rate_init = 0.029175888278947175, max_iter = 500, random_state=1)\n",
    "\n",
    "# Actually plugging in 12,3 gives us the sweet spot for Train accuracy, test accuracy, but more importantly an even precision and recall\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(12,3),activation = 'logistic', solver = 'sgd', max_iter = 500, random_state = 21)\n",
    " \n",
    "clf.fit(X_us, np.asarray(y_us))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Train Accuracy: \", clf.score(X_us, np.asarray(y_us)))\n",
    "print(\"Test Accuracy: \", clf.score(X_test, y_test))\n",
    "print(\"MSE Test: \", mean_squared_error(array(y_test), y_pred))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\n",
    "plt.xticks(ticks=np.arange(2) + 0.5 , labels=['No Response','Responds'])\n",
    "plt.yticks(ticks=np.arange(2) + 0.5, labels=['No Response','Responds'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names = ['No Response','Responds']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35486c7",
   "metadata": {},
   "source": [
    "## Trying Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4f57d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/96 [==============================] - 1s 3ms/step - loss: 0.5068 - accuracy: 0.7465 - mse: 0.1692 - val_loss: 0.9479 - val_accuracy: 0.4403 - val_mse: 0.3476\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.8033 - mse: 0.1358 - val_loss: 0.7020 - val_accuracy: 0.6017 - val_mse: 0.2477\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8033 - mse: 0.1332 - val_loss: 0.5549 - val_accuracy: 0.6992 - val_mse: 0.1884\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.8100 - mse: 0.1279 - val_loss: 0.6553 - val_accuracy: 0.6286 - val_mse: 0.2254\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8209 - mse: 0.1222 - val_loss: 0.6162 - val_accuracy: 0.6723 - val_mse: 0.2089\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3651 - accuracy: 0.8348 - mse: 0.1164 - val_loss: 0.5511 - val_accuracy: 0.7210 - val_mse: 0.1844\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8281 - mse: 0.1140 - val_loss: 0.6314 - val_accuracy: 0.7025 - val_mse: 0.2108\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8369 - mse: 0.1116 - val_loss: 0.4689 - val_accuracy: 0.7832 - val_mse: 0.1483\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3402 - accuracy: 0.8445 - mse: 0.1078 - val_loss: 0.7447 - val_accuracy: 0.5933 - val_mse: 0.2529\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8567 - mse: 0.1032 - val_loss: 0.5404 - val_accuracy: 0.7176 - val_mse: 0.1775\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8646 - mse: 0.0955 - val_loss: 0.4248 - val_accuracy: 0.8168 - val_mse: 0.1316\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8693 - mse: 0.0943 - val_loss: 0.4103 - val_accuracy: 0.8487 - val_mse: 0.1245\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.8794 - mse: 0.0892 - val_loss: 0.5169 - val_accuracy: 0.7899 - val_mse: 0.1667\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.8827 - mse: 0.0873 - val_loss: 0.2641 - val_accuracy: 0.9227 - val_mse: 0.0740\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8878 - mse: 0.0841 - val_loss: 0.4370 - val_accuracy: 0.8101 - val_mse: 0.1337\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8958 - mse: 0.0782 - val_loss: 0.5700 - val_accuracy: 0.7345 - val_mse: 0.1842\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.9054 - mse: 0.0724 - val_loss: 0.2721 - val_accuracy: 0.9176 - val_mse: 0.0749\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "#can definetely try keras keras tuning to target higher f1\n",
    "\n",
    "input_dnn = tf.keras.Input(shape=(X_os.shape[1],))\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(input_dnn)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "output_dnn = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=input_dnn, outputs=output_dnn)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        'mse'\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_os,\n",
    "    np.asarray(y_os),\n",
    "    validation_split=0.2,\n",
    "    batch_size=25,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48dd9f90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.45001\n",
      "Test Accuracy: 0.80%\n",
      "Test MSE: 0.14053\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(results[1]))\n",
    "print(\"Test MSE: {:.5f}\".format(results[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed115720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = np.array(model.predict(X_test) >= 0.5, dtype=np.int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "clr = classification_report(y_test, y_pred, target_names=['No Response','Responds'])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\n",
    "plt.xticks(ticks=np.arange(2) + 0.5, labels=['No Response','Responds'])\n",
    "plt.yticks(ticks=np.arange(2) + 0.5, labels=['No Response','Responds'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "594f7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow up work, hyper tuning using keras tuner and f-1 accuracy using hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e674188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Source obtained this method from https://neptune.ai/blog/keras-tuner-tuning-hyperparameters-deep-learning-model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def model_builder(hp):\n",
    "    \n",
    "#     # defining a set of hyperparametrs for tuning and a range of values for each\n",
    "    \n",
    "#     model = tf.keras.Model(inputs=input_dnn, outputs=output_dnn)\n",
    "#     model.add(keras.layers.Dense(\n",
    "#       hp.Choice('units', [8, 16, 32]),\n",
    "#       activation='relu'))\n",
    "    \n",
    "#     model.add(keras.layers.Dense(1, activation='relu'))\n",
    "    \n",
    "#     # model compilation\n",
    "#     model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy','f1',' precision','recall',]\n",
    "# )\n",
    "\n",
    "#     return model\n",
    "\n",
    "# tuner = kt.Hyperband(hypermodel = model_builder,\n",
    "#                      objective = kt.Objective(\"val_f1\", direction=\"max\"),\n",
    "#                      max_epochs = 20,\n",
    "#                      project_name='hyperband_tuner')\n",
    "\n",
    "\n",
    "\n",
    "# tuner.search(scaler.transform(X_os), np.asarray(y_os), epochs=100, validation_data=(X_test, y_test))\n",
    "\n",
    "        \n",
    "# tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdc51761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at mse graph and do k fold cross validation for avg mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9959cadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 00m 16s]\n",
      "val_f1_m: 0.6193476319313049\n",
      "\n",
      "Best val_f1_m So Far: 0.6462149620056152\n",
      "Total elapsed time: 00h 09m 42s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in ./hyperband_tuner17\n",
      "Showing 10 best trials\n",
      "Objective(name='val_f1_m', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 7\n",
      "units_0: 57\n",
      "activation: relu\n",
      "dropout: False\n",
      "lr: 0.00740837684630626\n",
      "units_1: 35\n",
      "units_2: 83\n",
      "dropout_rate: 0.465040968468216\n",
      "units_3: 132\n",
      "units_4: 121\n",
      "units_5: 140\n",
      "units_6: 139\n",
      "units_7: 26\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 50\n",
      "tuner/bracket: 4\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 292fff2e949b53e520715f8e3f1a9fd0\n",
      "Score: 0.6462149620056152\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 89\n",
      "activation: relu\n",
      "dropout: False\n",
      "lr: 0.01651516706560194\n",
      "units_1: 20\n",
      "units_2: 29\n",
      "dropout_rate: 0.5028994906471833\n",
      "units_3: 7\n",
      "units_4: 62\n",
      "units_5: 47\n",
      "units_6: 71\n",
      "units_7: 124\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 50\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: a17f70628a505d79a6e5344ac035a72c\n",
      "Score: 0.6406978964805603\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 35\n",
      "activation: tanh\n",
      "dropout: False\n",
      "lr: 0.06030364965628035\n",
      "units_1: 66\n",
      "units_2: 31\n",
      "dropout_rate: 0.6593336495918807\n",
      "units_3: 128\n",
      "units_4: 54\n",
      "units_5: 53\n",
      "units_6: 2\n",
      "units_7: 48\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 50\n",
      "tuner/bracket: 4\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 2bd9734fb3100145df93d6e7d36f681f\n",
      "Score: 0.6365955471992493\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 35\n",
      "activation: tanh\n",
      "dropout: False\n",
      "lr: 0.06030364965628035\n",
      "units_1: 66\n",
      "units_2: 31\n",
      "dropout_rate: 0.6593336495918807\n",
      "units_3: 128\n",
      "units_4: 54\n",
      "units_5: 53\n",
      "units_6: 2\n",
      "units_7: 48\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 4\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 23b017b69a0cd518bf93bcc4590f2850\n",
      "Score: 0.6342970132827759\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 129\n",
      "activation: relu\n",
      "dropout: False\n",
      "lr: 0.005659208579747373\n",
      "units_1: 56\n",
      "units_2: 123\n",
      "dropout_rate: 0.12137051393096997\n",
      "units_3: 146\n",
      "units_4: 19\n",
      "units_5: 123\n",
      "units_6: 63\n",
      "units_7: 2\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 3\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 95ed55886b70e6c7395518d5b31a8c1b\n",
      "Score: 0.6321285963058472\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 7\n",
      "units_0: 114\n",
      "activation: relu\n",
      "dropout: False\n",
      "lr: 0.005463823812582281\n",
      "units_1: 118\n",
      "units_2: 110\n",
      "dropout_rate: 0.468874021288247\n",
      "units_3: 76\n",
      "units_4: 104\n",
      "units_5: 132\n",
      "units_6: 2\n",
      "units_7: 48\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.6317230463027954\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 132\n",
      "activation: tanh\n",
      "dropout: False\n",
      "lr: 0.0014617452972265522\n",
      "units_1: 104\n",
      "units_2: 12\n",
      "dropout_rate: 0.550155443021729\n",
      "units_3: 129\n",
      "units_4: 100\n",
      "units_5: 48\n",
      "units_6: 99\n",
      "units_7: 74\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 7aa4307d0c4cf2e3971be6f783f895a2\n",
      "Score: 0.6298403739929199\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 63\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.006783057967741269\n",
      "units_1: 13\n",
      "units_2: 109\n",
      "dropout_rate: 0.14790408753027878\n",
      "units_3: 8\n",
      "units_4: 12\n",
      "units_5: 131\n",
      "units_6: 89\n",
      "units_7: 96\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 50\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 7f3a4de7c517d8b6980fb03dcfd9b1e3\n",
      "Score: 0.6284952759742737\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 146\n",
      "activation: sigmoid\n",
      "dropout: False\n",
      "lr: 0.02992368624767709\n",
      "units_1: 128\n",
      "units_2: 61\n",
      "dropout_rate: 0.32102665168813166\n",
      "units_3: 93\n",
      "units_4: 92\n",
      "units_5: 6\n",
      "units_6: 22\n",
      "units_7: 38\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.6270189881324768\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 89\n",
      "activation: relu\n",
      "dropout: False\n",
      "lr: 0.01651516706560194\n",
      "units_1: 20\n",
      "units_2: 29\n",
      "dropout_rate: 0.5028994906471833\n",
      "units_3: 7\n",
      "units_4: 62\n",
      "units_5: 47\n",
      "units_6: 71\n",
      "units_7: 124\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 7a0db2377d3b4a9f150f1cd8e73b40a3\n",
      "Score: 0.6227673292160034\n",
      "************************************************************************************************\n",
      "Results summary\n",
      "Results in ./hyperband_tunerOS\n",
      "Showing 10 best trials\n",
      "Objective(name='val_f1_m', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 134\n",
      "activation: sigmoid\n",
      "dropout: True\n",
      "lr: 0.007930273997838832\n",
      "units_1: 95\n",
      "units_2: 12\n",
      "units_3: 24\n",
      "units_4: 4\n",
      "units_5: 93\n",
      "units_6: 93\n",
      "dropout_rate: 0.278914187664643\n",
      "units_7: 144\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 50\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 64a8c88aab604f0efcb58539358b1186\n",
      "Score: 0.6406129598617554\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 102\n",
      "activation: relu\n",
      "dropout: False\n",
      "lr: 0.26565016302266886\n",
      "units_1: 27\n",
      "units_2: 1\n",
      "units_3: 11\n",
      "units_4: 139\n",
      "units_5: 69\n",
      "units_6: 61\n",
      "dropout_rate: 0.4135273874445281\n",
      "units_7: 77\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.6302170157432556\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 148\n",
      "activation: tanh\n",
      "dropout: True\n",
      "lr: 0.0015905683559190962\n",
      "units_1: 47\n",
      "units_2: 90\n",
      "units_3: 112\n",
      "units_4: 5\n",
      "units_5: 48\n",
      "units_6: 108\n",
      "dropout_rate: 0.0897962076576587\n",
      "units_7: 91\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 4\n",
      "tuner/round: 3\n",
      "tuner/trial_id: cfccf0735e766c66e9ddcaf133f731f4\n",
      "Score: 0.6230824589729309\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 127\n",
      "activation: sigmoid\n",
      "dropout: True\n",
      "lr: 0.013541114287165463\n",
      "units_1: 61\n",
      "units_2: 16\n",
      "units_3: 106\n",
      "units_4: 126\n",
      "units_5: 95\n",
      "units_6: 137\n",
      "dropout_rate: 0.4502540532249717\n",
      "units_7: 5\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.6195732355117798\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "units_0: 25\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.008181062284219479\n",
      "units_1: 109\n",
      "units_2: 125\n",
      "units_3: 107\n",
      "units_4: 114\n",
      "units_5: 77\n",
      "units_6: 24\n",
      "dropout_rate: 0.4138211470782113\n",
      "units_7: 24\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 50\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: a524ea62cba3d76af9d5440419d3c299\n",
      "Score: 0.6192628741264343\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 87\n",
      "activation: tanh\n",
      "dropout: True\n",
      "lr: 0.012699573351924797\n",
      "units_1: 115\n",
      "units_2: 30\n",
      "units_3: 69\n",
      "units_4: 77\n",
      "units_5: 132\n",
      "units_6: 83\n",
      "dropout_rate: 0.32748430204147\n",
      "units_7: 77\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 3\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 6ce82fafc54c7a1382c0a6f4d37ad40c\n",
      "Score: 0.6186725497245789\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 96\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.01069738718125143\n",
      "units_1: 70\n",
      "units_2: 75\n",
      "units_3: 108\n",
      "units_4: 91\n",
      "units_5: 142\n",
      "units_6: 11\n",
      "dropout_rate: 0.15938375000576924\n",
      "units_7: 46\n",
      "tuner/epochs: 6\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.6174860596656799\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 148\n",
      "activation: tanh\n",
      "dropout: True\n",
      "lr: 0.0015905683559190962\n",
      "units_1: 47\n",
      "units_2: 90\n",
      "units_3: 112\n",
      "units_4: 5\n",
      "units_5: 48\n",
      "units_6: 108\n",
      "dropout_rate: 0.0897962076576587\n",
      "units_7: 91\n",
      "tuner/epochs: 150\n",
      "tuner/initial_epoch: 50\n",
      "tuner/bracket: 4\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 9eac56797f1814a998d10103cf48fe67\n",
      "Score: 0.617054283618927\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 134\n",
      "activation: sigmoid\n",
      "dropout: True\n",
      "lr: 0.007930273997838832\n",
      "units_1: 95\n",
      "units_2: 12\n",
      "units_3: 24\n",
      "units_4: 4\n",
      "units_5: 93\n",
      "units_6: 93\n",
      "dropout_rate: 0.278914187664643\n",
      "units_7: 144\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: ec167248e2b10b5cb87ffecd46021f7e\n",
      "Score: 0.6155768036842346\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 131\n",
      "activation: relu\n",
      "dropout: False\n",
      "lr: 0.0032881072045554023\n",
      "units_1: 13\n",
      "units_2: 30\n",
      "units_3: 32\n",
      "units_4: 138\n",
      "units_5: 34\n",
      "units_6: 71\n",
      "dropout_rate: 0.3169859105687271\n",
      "units_7: 64\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: b91beced3ab6ff85ae0e73543cbc165b\n",
      "Score: 0.6128374338150024\n",
      "************************************************************************************************\n",
      "Results summary\n",
      "Results in ./hyperband_tunerUS\n",
      "Showing 10 best trials\n",
      "Objective(name='val_f1_m', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 143\n",
      "activation: tanh\n",
      "dropout: True\n",
      "lr: 0.005090297670558764\n",
      "units_1: 98\n",
      "dropout_rate: 0.2710414568336578\n",
      "units_2: 59\n",
      "units_3: 69\n",
      "units_4: 62\n",
      "units_5: 18\n",
      "units_6: 39\n",
      "units_7: 13\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: d1c8e05c0b789708c2083116501915c7\n",
      "Score: 0.6218975186347961\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 54\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.001578857419332992\n",
      "units_1: 126\n",
      "dropout_rate: 0.0831881259802745\n",
      "units_2: 59\n",
      "units_3: 119\n",
      "units_4: 5\n",
      "units_5: 101\n",
      "units_6: 126\n",
      "units_7: 86\n",
      "tuner/epochs: 6\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.6076093912124634\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 99\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.018396120703968125\n",
      "units_1: 20\n",
      "dropout_rate: 0.2256376389745258\n",
      "units_2: 17\n",
      "units_3: 126\n",
      "units_4: 138\n",
      "units_5: 141\n",
      "units_6: 26\n",
      "units_7: 89\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.6019285917282104\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 128\n",
      "activation: relu\n",
      "dropout: False\n",
      "lr: 0.006537746351401941\n",
      "units_1: 2\n",
      "dropout_rate: 0.3359211809444845\n",
      "units_2: 52\n",
      "units_3: 58\n",
      "units_4: 36\n",
      "units_5: 8\n",
      "units_6: 69\n",
      "units_7: 64\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.6013243794441223\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 54\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.001578857419332992\n",
      "units_1: 126\n",
      "dropout_rate: 0.0831881259802745\n",
      "units_2: 59\n",
      "units_3: 119\n",
      "units_4: 5\n",
      "units_5: 101\n",
      "units_6: 126\n",
      "units_7: 86\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 6e396ce8142018c0b062e425304c1f93\n",
      "Score: 0.6002344489097595\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 142\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.011755795695774021\n",
      "units_1: 94\n",
      "dropout_rate: 0.37865597776876275\n",
      "units_2: 132\n",
      "units_3: 61\n",
      "units_4: 46\n",
      "units_5: 89\n",
      "units_6: 122\n",
      "units_7: 21\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 4\n",
      "tuner/round: 3\n",
      "tuner/trial_id: aba115aeec018f9ed4aa3e4c491f3fe9\n",
      "Score: 0.5949764251708984\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 18\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.04057676346219411\n",
      "units_1: 124\n",
      "dropout_rate: 0.03507490979106212\n",
      "units_2: 92\n",
      "units_3: 130\n",
      "units_4: 75\n",
      "units_5: 32\n",
      "units_6: 110\n",
      "units_7: 14\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.5942949652671814\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 54\n",
      "activation: relu\n",
      "dropout: True\n",
      "lr: 0.001578857419332992\n",
      "units_1: 126\n",
      "dropout_rate: 0.0831881259802745\n",
      "units_2: 59\n",
      "units_3: 119\n",
      "units_4: 5\n",
      "units_5: 101\n",
      "units_6: 126\n",
      "units_7: 86\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 3\n",
      "tuner/round: 1\n",
      "tuner/trial_id: a27086d93608e12b851f4455eae9980e\n",
      "Score: 0.5912259221076965\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 91\n",
      "activation: tanh\n",
      "dropout: True\n",
      "lr: 0.00904336009515485\n",
      "units_1: 12\n",
      "dropout_rate: 0.002287192467757226\n",
      "units_2: 24\n",
      "units_3: 8\n",
      "units_4: 50\n",
      "units_5: 47\n",
      "units_6: 44\n",
      "units_7: 5\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 2ad2d33b502da2ae696bc2d0a1f4271e\n",
      "Score: 0.5893950462341309\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 47\n",
      "activation: tanh\n",
      "dropout: True\n",
      "lr: 0.06955756363025771\n",
      "units_1: 79\n",
      "dropout_rate: 0.37128610594335754\n",
      "units_2: 77\n",
      "units_3: 101\n",
      "units_4: 21\n",
      "units_5: 13\n",
      "units_6: 96\n",
      "units_7: 34\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: f2d9e7b669cc93092cb4275b9b5b060e\n",
      "Score: 0.5877626538276672\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 8)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=1, max_value=150),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\",\"sigmoid\"]),\n",
    "            )\n",
    "        )\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate= hp.Float(\"dropout_rate\",min_value = 0.0, max_value = 0.7)))\n",
    "    learning_rate = hp.Float(\"lr\",0.001,0.7, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[f1_m],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# tuner initialization\n",
    "betterfs1tuner = kt.Hyperband(hypermodel = build_model,\n",
    "                     objective = kt.Objective(\"val_f1_m\", direction=\"max\"),\n",
    "                     max_epochs = 150,\n",
    "                     project_name='hyperband_tuner17')\n",
    "\n",
    "betterfs1tunerOS = kt.Hyperband(hypermodel = build_model,\n",
    "                     objective = kt.Objective(\"val_f1_m\", direction=\"max\"),\n",
    "                     max_epochs = 150,\n",
    "                     project_name='hyperband_tunerOS')\n",
    "\n",
    "betterfs1tunerUS = kt.Hyperband(hypermodel = build_model,\n",
    "                     objective = kt.Objective(\"val_f1_m\", direction=\"max\"),\n",
    "                     max_epochs = 150,\n",
    "                     project_name='hyperband_tunerUS')\n",
    "\n",
    "betterfs1tuner.search(X_train, y_train, epochs=150, validation_data=(X_test, y_test))\n",
    "\n",
    "betterfs1tunerOS.search(X_os, y_os, epochs=150, validation_data=(X_test, y_test))\n",
    "\n",
    "betterfs1tunerUS.search(X_us, y_us, epochs=150, validation_data=(X_test, y_test))\n",
    "\n",
    "betterfs1tuner.results_summary()\n",
    "\n",
    "print(\"************************************************************************************************\")\n",
    "\n",
    "betterfs1tunerOS.results_summary()\n",
    "\n",
    "print(\"************************************************************************************************\")\n",
    "\n",
    "betterfs1tunerUS.results_summary()\n",
    "\n",
    "\n",
    "# Best hp for fs1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81909bd",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "num_layers: 1\n",
    "units_0: 52\n",
    "activation: relu\n",
    "dropout: False\n",
    "lr: 0.0018084480464569503\n",
    "units_1: 3\n",
    "units_2: 19\n",
    "units_3: 24\n",
    "dropout_rate: 0.35000000000000003\n",
    "tuner/epochs: 3\n",
    "tuner/initial_epoch: 0\n",
    "tuner/bracket: 2\n",
    "tuner/round: 0\n",
    "Score: 0.7182466387748718"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4db766",
   "metadata": {},
   "source": [
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(60, input_dim=60, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tsgd = SGD(lr=0.1, momentum=0.9)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ad3b99df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4884 - f1_m: 0.0000e+00 - val_loss: 0.3182 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4067 - f1_m: 0.0000e+00 - val_loss: 0.2996 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3735 - f1_m: 0.0000e+00 - val_loss: 0.2639 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3435 - f1_m: 0.0185 - val_loss: 0.2560 - val_f1_m: 0.1852\n",
      "Epoch 5/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3106 - f1_m: 0.1417 - val_loss: 0.2297 - val_f1_m: 0.1852\n",
      "Epoch 6/30\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2922 - f1_m: 0.3528 - val_loss: 0.2302 - val_f1_m: 0.1852\n",
      "Epoch 7/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2806 - f1_m: 0.3264 - val_loss: 0.7690 - val_f1_m: 0.2376\n",
      "Epoch 8/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2846 - f1_m: 0.4176 - val_loss: 0.2257 - val_f1_m: 0.1852\n",
      "Epoch 9/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2626 - f1_m: 0.4083 - val_loss: 0.2406 - val_f1_m: 0.2593\n",
      "Epoch 10/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2608 - f1_m: 0.4111 - val_loss: 0.2446 - val_f1_m: 0.2593\n",
      "Epoch 11/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2472 - f1_m: 0.3972 - val_loss: 0.2434 - val_f1_m: 0.1852\n",
      "Epoch 12/30\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2542 - f1_m: 0.4713 - val_loss: 0.2540 - val_f1_m: 0.2593\n",
      "Epoch 13/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2550 - f1_m: 0.4188 - val_loss: 0.2511 - val_f1_m: 0.2593\n",
      "Epoch 14/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2536 - f1_m: 0.4375 - val_loss: 0.2696 - val_f1_m: 0.2593\n",
      "Epoch 15/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2522 - f1_m: 0.4380 - val_loss: 0.8940 - val_f1_m: 0.2208\n",
      "Epoch 16/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2776 - f1_m: 0.5216 - val_loss: 0.2635 - val_f1_m: 0.2593\n",
      "Epoch 17/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2410 - f1_m: 0.4917 - val_loss: 0.2481 - val_f1_m: 0.1852\n",
      "Epoch 18/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2391 - f1_m: 0.4964 - val_loss: 0.2934 - val_f1_m: 0.2741\n",
      "Epoch 19/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2383 - f1_m: 0.4872 - val_loss: 0.2582 - val_f1_m: 0.2593\n",
      "Epoch 20/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2328 - f1_m: 0.4287 - val_loss: 0.2657 - val_f1_m: 0.2593\n",
      "Epoch 21/30\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2330 - f1_m: 0.4955 - val_loss: 0.2900 - val_f1_m: 0.2741\n",
      "Epoch 22/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2375 - f1_m: 0.4025 - val_loss: 0.2838 - val_f1_m: 0.2741\n",
      "Epoch 23/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2209 - f1_m: 0.4581 - val_loss: 0.2695 - val_f1_m: 0.2593\n",
      "Epoch 24/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2221 - f1_m: 0.4485 - val_loss: 0.2732 - val_f1_m: 0.1852\n",
      "Epoch 25/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2155 - f1_m: 0.4269 - val_loss: 0.3172 - val_f1_m: 0.2741\n",
      "Epoch 26/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2211 - f1_m: 0.5907 - val_loss: 0.3098 - val_f1_m: 0.2741\n",
      "Epoch 27/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2274 - f1_m: 0.5627 - val_loss: 0.2714 - val_f1_m: 0.2593\n",
      "Epoch 28/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2188 - f1_m: 0.4736 - val_loss: 0.3055 - val_f1_m: 0.2407\n",
      "Epoch 29/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2073 - f1_m: 0.5000 - val_loss: 0.2778 - val_f1_m: 0.2593\n",
      "Epoch 30/30\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2192 - f1_m: 0.5426 - val_loss: 0.2945 - val_f1_m: 0.2593\n",
      "Epoch 1/30\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6343 - f1_m: 0.1312 - val_loss: 0.4676 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4889 - f1_m: 0.0741 - val_loss: 0.3829 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4708 - f1_m: 0.0139 - val_loss: 0.3578 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4476 - f1_m: 0.0111 - val_loss: 0.3373 - val_f1_m: 0.0000e+00\n",
      "Epoch 5/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4565 - f1_m: 0.0000e+00 - val_loss: 0.3346 - val_f1_m: 0.0000e+00\n",
      "Epoch 6/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4413 - f1_m: 0.0278 - val_loss: 0.3257 - val_f1_m: 0.0000e+00\n",
      "Epoch 7/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4269 - f1_m: 0.0000e+00 - val_loss: 0.3216 - val_f1_m: 0.0000e+00\n",
      "Epoch 8/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4604 - f1_m: 0.0000e+00 - val_loss: 0.3225 - val_f1_m: 0.0000e+00\n",
      "Epoch 9/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4507 - f1_m: 0.0000e+00 - val_loss: 0.3210 - val_f1_m: 0.0000e+00\n",
      "Epoch 10/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4432 - f1_m: 0.0000e+00 - val_loss: 0.3246 - val_f1_m: 0.0000e+00\n",
      "Epoch 11/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4507 - f1_m: 0.0000e+00 - val_loss: 0.3206 - val_f1_m: 0.0000e+00\n",
      "Epoch 12/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4542 - f1_m: 0.0000e+00 - val_loss: 0.3240 - val_f1_m: 0.0000e+00\n",
      "Epoch 13/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4476 - f1_m: 0.0000e+00 - val_loss: 0.3263 - val_f1_m: 0.0000e+00\n",
      "Epoch 14/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4315 - f1_m: 0.0000e+00 - val_loss: 0.3235 - val_f1_m: 0.0000e+00\n",
      "Epoch 15/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4549 - f1_m: 0.0000e+00 - val_loss: 0.3209 - val_f1_m: 0.0000e+00\n",
      "Epoch 16/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4635 - f1_m: 0.0000e+00 - val_loss: 0.3212 - val_f1_m: 0.0000e+00\n",
      "Epoch 17/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4558 - f1_m: 0.0000e+00 - val_loss: 0.3196 - val_f1_m: 0.0000e+00\n",
      "Epoch 18/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4372 - f1_m: 0.0000e+00 - val_loss: 0.3182 - val_f1_m: 0.0000e+00\n",
      "Epoch 19/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4545 - f1_m: 0.0000e+00 - val_loss: 0.3177 - val_f1_m: 0.0000e+00\n",
      "Epoch 20/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4568 - f1_m: 0.0000e+00 - val_loss: 0.3177 - val_f1_m: 0.0000e+00\n",
      "Epoch 21/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4422 - f1_m: 0.0000e+00 - val_loss: 0.3174 - val_f1_m: 0.0000e+00\n",
      "Epoch 22/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4608 - f1_m: 0.0000e+00 - val_loss: 0.3183 - val_f1_m: 0.0000e+00\n",
      "Epoch 23/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4437 - f1_m: 0.0000e+00 - val_loss: 0.3179 - val_f1_m: 0.0000e+00\n",
      "Epoch 24/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4376 - f1_m: 0.0000e+00 - val_loss: 0.3179 - val_f1_m: 0.0000e+00\n",
      "Epoch 25/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4323 - f1_m: 0.0000e+00 - val_loss: 0.3187 - val_f1_m: 0.0000e+00\n",
      "Epoch 26/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4537 - f1_m: 0.0000e+00 - val_loss: 0.3188 - val_f1_m: 0.0000e+00\n",
      "Epoch 27/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4350 - f1_m: 0.0000e+00 - val_loss: 0.3180 - val_f1_m: 0.0000e+00\n",
      "Epoch 28/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4279 - f1_m: 0.0000e+00 - val_loss: 0.3165 - val_f1_m: 0.0000e+00\n",
      "Epoch 29/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4449 - f1_m: 0.0000e+00 - val_loss: 0.3175 - val_f1_m: 0.0000e+00\n",
      "Epoch 30/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4376 - f1_m: 0.0000e+00 - val_loss: 0.3175 - val_f1_m: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5673 - f1_m: 0.0562 - val_loss: 0.4148 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4431 - f1_m: 0.0000e+00 - val_loss: 0.3375 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.4101 - f1_m: 0.0000e+00 - val_loss: 0.3137 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3939 - f1_m: 0.0000e+00 - val_loss: 0.3012 - val_f1_m: 0.0000e+00\n",
      "Epoch 5/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3838 - f1_m: 0.0000e+00 - val_loss: 0.2950 - val_f1_m: 0.0000e+00\n",
      "Epoch 6/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3750 - f1_m: 0.0000e+00 - val_loss: 0.2858 - val_f1_m: 0.0000e+00\n",
      "Epoch 7/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3616 - f1_m: 0.0000e+00 - val_loss: 0.2810 - val_f1_m: 0.0000e+00\n",
      "Epoch 8/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3600 - f1_m: 0.0000e+00 - val_loss: 0.2946 - val_f1_m: 0.1111\n",
      "Epoch 9/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3444 - f1_m: 0.0185 - val_loss: 0.2771 - val_f1_m: 0.1111\n",
      "Epoch 10/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3437 - f1_m: 0.0000e+00 - val_loss: 0.2692 - val_f1_m: 0.1111\n",
      "Epoch 11/30\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3232 - f1_m: 0.0556 - val_loss: 0.2663 - val_f1_m: 0.1111\n",
      "Epoch 12/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3288 - f1_m: 0.0463 - val_loss: 0.2642 - val_f1_m: 0.1111\n",
      "Epoch 13/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3222 - f1_m: 0.1546 - val_loss: 0.2593 - val_f1_m: 0.1852\n",
      "Epoch 14/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3211 - f1_m: 0.0852 - val_loss: 0.2644 - val_f1_m: 0.1852\n",
      "Epoch 15/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3080 - f1_m: 0.1284 - val_loss: 0.2571 - val_f1_m: 0.1852\n",
      "Epoch 16/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3089 - f1_m: 0.1731 - val_loss: 0.2537 - val_f1_m: 0.1852\n",
      "Epoch 17/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3047 - f1_m: 0.2120 - val_loss: 0.2508 - val_f1_m: 0.1852\n",
      "Epoch 18/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3011 - f1_m: 0.2222 - val_loss: 0.2502 - val_f1_m: 0.1852\n",
      "Epoch 19/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2914 - f1_m: 0.2083 - val_loss: 0.2497 - val_f1_m: 0.1852\n",
      "Epoch 20/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2924 - f1_m: 0.2070 - val_loss: 0.2720 - val_f1_m: 0.1852\n",
      "Epoch 21/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2915 - f1_m: 0.2909 - val_loss: 0.2463 - val_f1_m: 0.1852\n",
      "Epoch 22/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2937 - f1_m: 0.2917 - val_loss: 0.2453 - val_f1_m: 0.1852\n",
      "Epoch 23/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2896 - f1_m: 0.3296 - val_loss: 0.2543 - val_f1_m: 0.1852\n",
      "Epoch 24/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2876 - f1_m: 0.3213 - val_loss: 0.2457 - val_f1_m: 0.1852\n",
      "Epoch 25/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2834 - f1_m: 0.3176 - val_loss: 0.2485 - val_f1_m: 0.1852\n",
      "Epoch 26/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2812 - f1_m: 0.3131 - val_loss: 0.2449 - val_f1_m: 0.1852\n",
      "Epoch 27/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2847 - f1_m: 0.2509 - val_loss: 0.2693 - val_f1_m: 0.1852\n",
      "Epoch 28/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2808 - f1_m: 0.3574 - val_loss: 0.2523 - val_f1_m: 0.1852\n",
      "Epoch 29/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2725 - f1_m: 0.3423 - val_loss: 0.2469 - val_f1_m: 0.1852\n",
      "Epoch 30/30\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2701 - f1_m: 0.3645 - val_loss: 0.2453 - val_f1_m: 0.1852\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "# Original dataset model\n",
    "input_dnn = tf.keras.Input(shape=(X_train.shape[1],))\n",
    "x = tf.keras.layers.Dense(38, input_dim = 38, activation='relu')(input_dnn)\n",
    "x = tf.keras.layers.Dense(65, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(63, activation='relu')(x)\n",
    "output_dnn = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=input_dnn, outputs=output_dnn)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer= SGD(learning_rate= 0.05634684341799796),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        f1_m\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_test,\n",
    "    np.asarray(y_test),\n",
    "    validation_split=0.2,\n",
    "    batch_size=10,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=30,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Oversampled model\n",
    "\n",
    "input_dnnOS = tf.keras.Input(shape=(X_os.shape[1],))\n",
    "y = tf.keras.layers.Dense(134, input_dim = 134, activation='sigmoid')(input_dnnOS)\n",
    "y = tf.keras.layers.Dense(95, activation='sigmoid')(y)\n",
    "y = tf.keras.layers.Dense(24, activation='sigmoid')(y)\n",
    "y = tf.keras.layers.Dropout(0.278914187664643)(y)\n",
    "output_dnnOS = tf.keras.layers.Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "\n",
    "modelOS = tf.keras.Model(inputs=input_dnnOS, outputs=output_dnnOS)\n",
    "\n",
    "\n",
    "modelOS.compile(\n",
    "    optimizer= SGD(learning_rate= 0.007930273997838832),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        f1_m\n",
    "    ]\n",
    ")\n",
    "\n",
    "historyOS = modelOS.fit(\n",
    "    X_test,\n",
    "    np.asarray(y_test),\n",
    "    validation_split=0.2,\n",
    "    batch_size=10,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=30,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Undersampled\n",
    "\n",
    "input_dnnUS = tf.keras.Input(shape=(X_us.shape[1],))\n",
    "z = tf.keras.layers.Dense(143, input_dim = 143, activation='tanh')(input_dnnUS)\n",
    "z = tf.keras.layers.Dense(98, activation='tanh')(z)\n",
    "z = tf.keras.layers.Dense(59, activation='tanh')(z)\n",
    "z = tf.keras.layers.Dropout(0.2710414568336578)(z)\n",
    "output_dnnUS = tf.keras.layers.Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "\n",
    "modelUS = tf.keras.Model(inputs=input_dnnUS, outputs=output_dnnUS)\n",
    "\n",
    "\n",
    "modelUS.compile(\n",
    "    optimizer= SGD(learning_rate= 0.005090297670558764),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        f1_m\n",
    "    ]\n",
    ")\n",
    "\n",
    "historyUS = modelUS.fit(\n",
    "    X_test,\n",
    "    np.asarray(y_test),\n",
    "    validation_split=0.2,\n",
    "    batch_size=10,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=30,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f518ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.21327\n",
      "Test F1: 0.63783%\n",
      "Test Loss: 0.40308\n",
      "Test F1: 0.00000%\n",
      "Test Loss: 0.26376\n",
      "Test F1: 0.50041%\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test F1: {:.5f}%\".format(results[1]))\n",
    "# print(\"Test MSE: {:.5f}\".format(results[2]))\n",
    "\n",
    "results = modelOS.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test F1: {:.5f}%\".format(results[1]))\n",
    "\n",
    "results = modelUS.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test F1: {:.5f}%\".format(results[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413e3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project_encoding_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
